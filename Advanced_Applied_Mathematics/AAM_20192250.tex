\documentclass[12pt,openany]{book}

\usepackage{amsmath,amsthm,amsfonts,amscd} % Packages for mathematics
\usepackage{commath}

% Colors
\usepackage[dvipsnames]{xcolor}
\definecolor{titleblue}{RGB}{0,53,128}
\definecolor{chaptergray}{RGB}{140,140,140}
\definecolor{sectiongray}{RGB}{180,180,180}

\definecolor{thmcolor}{RGB}{231, 76, 60}
\definecolor{defcolor}{RGB}{52, 152, 219}
\definecolor{lemcolor}{RGB}{155, 89, 182}
\definecolor{corcolor}{RGB}{46, 204, 113}
\definecolor{procolor}{RGB}{241, 196, 15}

% Fonts
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{newpxtext,newpxmath}
\usepackage{sectsty}
\allsectionsfont{\sffamily\color{titleblue}\mdseries}

% Page layout
\usepackage{geometry}
\geometry{a4paper,left=1.2in,right=.7in,top=1in,bottom=1in,heightrounded}
\usepackage{fancyhdr}
\fancyhf{}
\fancyhead[LE,RO]{\thepage}
\fancyhead[LO]{\nouppercase{\rightmark}}
\fancyhead[RE]{\nouppercase{\leftmark}}
\renewcommand{\headrulewidth}{0.5pt}
\renewcommand{\footrulewidth}{0pt}

% Chapter formatting
\usepackage{titlesec}
\titleformat{\chapter}[display]
{\normalfont\sffamily\Huge\bfseries\color{titleblue}}{\chaptertitlename\ \thechapter}{20pt}{\Huge}
\titleformat{\section}
{\normalfont\sffamily\Large\bfseries\color{titleblue!100!gray}}{\thesection}{1em}{}
\titleformat{\subsection}
{\normalfont\sffamily\large\bfseries\color{titleblue!75!gray}}{\thesubsection}{1em}{}

% Table of contents formatting
\usepackage{tocloft}
\renewcommand{\cftchapfont}{\sffamily\color{titleblue}\bfseries}
\renewcommand{\cftsecfont}{\sffamily\color{chaptergray}}
\renewcommand{\cftsubsecfont}{\sffamily\color{sectiongray}}
\renewcommand{\cftchapleader}{\cftdotfill{\cftdotsep}}

% Hyperlinks
\usepackage{hyperref}
\hypersetup{
	colorlinks=true,
	linkcolor=titleblue,
	filecolor=black,      
	urlcolor=titleblue,
}

%Listing
\usepackage{listings} %Code
\renewcommand{\lstlistingname}{Code}%

\definecolor{sagegreen}{rgb}{0.0,0.6,0.4}
\definecolor{sagepurple}{rgb}{0.6,0.0,0.4}
\definecolor{sageblue}{rgb}{0.0,0.4,0.6}
\definecolor{sageorange}{rgb}{1.0,0.4,0.0}
\definecolor{sagegray}{rgb}{0.4,0.4,0.4}

\lstdefinestyle{sage}{
	language=Python,
	backgroundcolor=\color{white},
	basicstyle=\small\ttfamily\color{black}, 
	basicstyle=\footnotesize\ttfamily\color{black},
	keywordstyle=\color{blue!60!black},
	commentstyle=\color{green!60!black},
	stringstyle=\color{purple!60!black},
	showstringspaces=false,
	breaklines=true,
	tabsize=4,
	morekeywords={True, False, None},
	frame=leftline, % Remove the border
	framesep=3pt,
	frameround=tttt,
	framexleftmargin=3pt,
	numbers=left,
	numberstyle=\small\color{gray},
	xleftmargin=15pt, % Increase the left margin
	xrightmargin=5pt,
	captionpos=b,
	belowskip=0pt,
	aboveskip=4pt
}

%Ceiling and Floor Function
\usepackage{mathtools}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\DeclarePairedDelimiter{\floor}{\lfloor}{\rfloor}

%Algorithm
\usepackage[ruled,linesnumbered]{algorithm2e}
\usepackage{setspace}
\usepackage{algpseudocode}
\SetKwComment{Comment}{/* }{ */}
\SetKw{Break}{break}
\SetKw{Downto}{downto}
\SetKwProg{Fn}{Function}{:}{end}
\SetKwFunction{KeyGen}{KeyGen}


%---------------------------My Preamble
\usepackage{marvosym} %Lightning
\usepackage{booktabs}
\usepackage{multicol}
\setlength{\columnsep}{2cm}
\setlength{\columnseprule}{1.25pt}
\usepackage{enumerate}
\usepackage{soul}
\newcommand{\mathcolorbox}[2]{\colorbox{#1}{$\displaystyle #2$}}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{tikz-cd}
\usetikzlibrary{calc}
\usetikzlibrary{arrows, arrows.meta, positioning, shapes.multipart}

%Tcolorbox
\usepackage[most]{tcolorbox}
\tcbset{colback=white, arc=5pt}
%\tcbset{enhanced, colback=white,colframe=black,fonttitle=\bfseries,arc=4mm,boxrule=1pt,shadow={2mm}{-1mm}{0mm}{black!50}}
%White box with black text and shadow
%\begin{tcolorbox}[colback=white,colframe=black,fonttitle=\bfseries,title=Black Shadow Box,arc=4mm,boxrule=1pt,shadow={2mm}{-1mm}{0mm}{black!50}]
%	This is a white box with black text and a subtle shadow. The shadow adds some depth and dimension to the box without overpowering the design.
%\end{tcolorbox}

%Theorem
\newtheorem{axiom}{Axiom}[chapter]
\newtheorem{theorem}{Theorem}[chapter]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}

\theoremstyle{definition}
\newtheorem{definition}{Definition}[chapter]
\newtheorem{remark}{Remark}[chapter]
\newtheorem{exercise}{Exercise}[chapter]
\newtheorem{example}{Example}[chapter]
\newtheorem*{note}{Note}

%New Command
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\F}{\mathbb{F}}

\newcommand{\ie}{\textnormal{i.e.}}
\newcommand{\eg}{\textnormal{e.g.}}

\newcommand{\of}[1]{\left( #1 \right)} 

\newcommand{\nbhd}{\mathcal{N}}
\newcommand{\Id}{\operatorname{\textnormal{id}}}

%\newcommand{\norm}[1]{\left\| #1 \right\|}

\newcommand{\sol}{\textcolor{magenta}{\bf Sol}}

\newcommand{\inv}[1]{{#1}^{-1}}
\newcommand{\img}{\textnormal{Im}}

\newcommand{\by}{\times}
\newcommand{\Span}[1]{\textnormal{span}\langle #1\rangle}
\newcommand{\Sspan}[1]{\textnormal{span}\bigg\langle #1\bigg\rangle}
\newcommand{\basis}{\mathscr{B}}
\newcommand{\scrC}{\mathscr{C}}
\newcommand{\rank}{\textnormal{rank}}
\newcommand{\inner}[1]{\langle #1\rangle}
\newcommand{\norms}[1]{|| #1||}
\newcommand{\tr}{\textnormal{tr}}
\newcommand{\conjugate}[1]{\overline{#1}}


\renewcommand{\vec}[1]{\textbf{#1}}



% Begin document
\begin{document}
	
	% Title page
	\begin{titlepage}
		\begin{center}
			{\Huge\textsf{\textbf{Advanced Applied Mathematics}}\par}
			\vspace{0.5in}
			{\Large Ji Yong-Hyeon\par}
			\vspace{1in}
			\includegraphics[scale=.5]{aam.png}\par
			\vspace{1in}
			{Department of Information Security, Cryptology, and Mathematics\par}
			{College of Science and Technology\par}
			{Kookmin University\par}
			%\includegraphics[width=1.5in]{school_logo.jpg}\par
			\vspace{.25in}
			{\large \today\par}
		\end{center}
	\end{titlepage}
	
	% Table of contents
	\tableofcontents
	
	% Chapters
	\mainmatter
	
	\chapter{Linear Algebra}
	\section{System of Linear Equations}
	\begin{itemize}
		\item A system of linear equations
		\[
		\begin{cases}
			x_1,\dots,x_n:\text{unknowns}\\
			\text{\# of unknowns}=n\\
			\text{\# of equations}=m
		\end{cases}
		\]
		\begin{align*}
			&\begin{cases}
				a_{11}x_1+a_{12}x_2+\cdots+a_{1n}x_n=b_1\\
				a_{21}x_1+a_{22}x_2+\cdots+a_{2n}x_n=b_2\\
				\hspace{50pt}\vdots\\
				a_{m1}x_1+a_{m2}x_2+\cdots+a_{mn}x_n=b_m\\
			\end{cases}\\
			\iff&\begin{bmatrix}
				a_{11} & a_{12} & \cdots & a_{1n}\\
				\vdots \\
				a_{m1} & a_{m2} &\cdots & a_{mn}
			\end{bmatrix}\begin{bmatrix}
				x_1\\ \vdots \\ x_n
			\end{bmatrix}=\begin{bmatrix}
				b_1\\ \vdots \\ b_m
			\end{bmatrix} &\textcolor{blue}{A\textbf{x}=\textbf{b}}\\
			\iff& x_1\begin{bmatrix}
				a_{11}\\ a_{21}\\ \vdots\\ a_{m1}
			\end{bmatrix}+x_2\begin{bmatrix}
				a_{12}\\ a_{22}\\ \vdots\\ a_{m2}
			\end{bmatrix}+\cdots+x_n\begin{bmatrix}
				a_{1m}\\ a_{2m}\\ \vdots\\ a_{mn}
			\end{bmatrix}=\begin{bmatrix}
				b_1\\ b_2\\ \vdots\\ b_{m} 
			\end{bmatrix} & \textcolor{blue}{x_1\textbf{C}_1+\cdots+x_n\textbf{C}_n=\textbf{b}}
		\end{align*}
		\item Matrix operation
		\begin{enumerate}[(i)]
			\item scalar multiplication: $kA$
			\item addition: $A+B$
			\item multiplication: $AB$
		\end{enumerate}
		\item Properties \begin{itemize}
			\item Associative: $(A+B)+C=A+(B+C)$, $A(BC)=(AB)C$
			\item Distributive: $(AB)C=A(BC)$
			\item (in general) not commutative: $AB\neq BA$
		\end{itemize}
		\item Transpose of $A$: $A^T$ \[
		(a_{ij})_{m\times n}\longrightarrow(a^t_{ij})_{n\times m}=(a_{ji})_{n\times m}
		\]
		\item Square Matrices
	\end{itemize}
	
	\subsection{Elementary Transformation}
	\begin{itemize}
		\item Exchange of two equations (rows in the matrix representing the system
		of equations)
		\item Multiplication of an equation (row) with a constant $\lambda\in\R^*$
		\item Addition of two equations (rows)
	\end{itemize}
	
	\begin{remark}
		$A\textbf{x}=\textbf{b}\iff[A\mid\textbf{b}]$.
	\end{remark}
	
	\begin{example}
		\begin{align*}
			\begin{bmatrix}
				1 & 1 & 1\\ 1 & -1& 2\\ 2 & 0 & 3
			\end{bmatrix}\begin{bmatrix}
				x_1\\ x_2\\ x_3
			\end{bmatrix}=\begin{bmatrix}
				3 \\ 2 \\ 5
			\end{bmatrix} &\iff \left[
			\begin{array}{ccc|c}
				1 & 1 & 1 & 3\\
				1 & -1 & 2 & 2\\
				2 & 0 & 3 & 5
			\end{array}
			\right] \\ &\xLeftrightarrow[R_3\gets R_3-2R_1]{R_2\gets R_2-R_1} \left[
			\begin{array}{ccc|c}
				1 & 1 & 1 & 3\\
				0 & -2 & 1 & -1\\
				0 & -2 & 1 & -1
			\end{array} 
			\right] \\
			&\xLeftrightarrow[R_2\gets -\frac{1}{2}R_2]{R_3\gets R_3-R_2} \left[
			\begin{array}{ccc|c}
				1 & 1 & 1 & 3\\
				0 & 1 & -1/2 & 1/2\\
				0 & 0 & 0 & 0
			\end{array} 
			\right]\quad\text{Row-Echelon Form (REF)} \\
			&\xLeftrightarrow{R_1\gets R_1-R_2} \left[
			\begin{array}{ccc|c}
				1 & 0 & 3/2 & 5/2\\
				0 & 1 & -1/2 & 1/2\\
				0 & 0 & 0 & 0
			\end{array} 
			\right]\quad\text{Reduced Row-Echelon Form (RREF)} \\
			&\iff \begin{cases}
				x_1=-\frac{3}{2}x_3+\frac{5}{2}\\
				x_2=\frac{1}{2}x_3+\frac{1}{2}
			\end{cases}.
		\end{align*} Let $x_3=\lambda$ then \[
		\textbf{x}=\begin{bmatrix}
			-\frac{3}{2}\lambda+\frac{5}{2}\\ \frac{1}{2}\lambda+\frac{1}{2}\\ \lambda
		\end{bmatrix}=\begin{bmatrix}
		\frac{5}{2}\\ \frac{1}{2}\\ 0
		\end{bmatrix}+\lambda\begin{bmatrix}
			-\frac{3}{2}\\ \frac{1}{2}\\ 1
		\end{bmatrix}.
		\]
	\end{example}

	\section{Reduced Row-Echelon Form (RREF)}
	\section{General Solution and Minus-1 Trick}
	
	\section{Vector Space, Linear Transformation, Basis}
	
	\newpage
	\section{Matrix Representation of Linear Mappings}
	
	\section{Basis and Rank}
	\section{Linear Mappings}
	\subsection{Basis Change}
	
	\begin{tcolorbox}[colframe=thmcolor,title={\color{white}\bf Basis Change}]
		\begin{theorem}
			For a linear mapping $\Phi:V\to W$, ordered bases \[
			\mathscr{B}=(\textbf{b}_1,\dots,\textbf{b}_n),\quad\tilde{\mathscr{B}}=(\tilde{\textbf{b}}_1,\cdots\tilde{\textbf{b}}_n)
			\] of \(V\) and \[
			\mathscr{C}=(\textbf{c}_1,\dots,\textbf{c}_m),\quad\tilde{\mathscr{C}}=(\tilde{\textbf{c}}_1,\cdots\tilde{\textbf{c}}_m)
			\] of \(W\), and a transformation matrix \(\textbf{A}_{\Phi}=\sbr[1]{a_{ij}}_{m\by n}\) w.r.t. \(\mathscr{B}\) and \(\mathscr{C}\), the corresponding transformation matrix \(\tilde{\textbf{A}}_\Phi=\sbr[1]{\tilde{a}_{ij}}_{m\by n}\) w.r.t. the bases \(\tilde{\mathscr{B}}\) and \(\tilde{\mathscr{C}}\) is given \[
			\boxed{\tilde{\textbf{A}}_\Phi=\textbf{T}^{-1}\textbf{A}_\Phi \textbf{S}}.
			\]
			\begin{tikzcd}
				&& V \arrow[rr, "\Phi"] && W && V \arrow[rr, "\Phi"] && W\\
				&& \mathscr{B} \arrow[rr, "\textbf{A}_\Phi"] && \mathscr{C} && \mathscr{B} \arrow[rr, "\textbf{A}_\Phi"] && \mathscr{C} \arrow[dd, "\textbf{T}^{-1}"] \\
				&& && && &&\\
				&& \tilde{\mathscr{B}} \arrow[uu, "\textbf{S}"] \arrow[rr, "\tilde{\textbf{A}}_\Phi"] && \tilde{\mathscr{C}} \arrow[uu, "\textbf{T}"'] && \tilde{\mathscr{B}} \arrow[uu, "\textbf{S}"] \arrow[rr, "\tilde{\textbf{A}}_\Phi"] && \tilde{\mathscr{C}}             
			\end{tikzcd}
		\end{theorem}
	\end{tcolorbox}
	\begin{proof}
		Let \[
			\textbf{S}:=\sbr[1]{s_{ij}}_{n\by n}=\sbr[2]{\tilde{\textbf{b}}_1\ \tilde{\textbf{b}}_2\ \cdots\ \tilde{\textbf{b}}_n}_{\basis},\quad
			\text{and}\quad
			\textbf{T}:=\sbr[1]{t_{lk}}_{m\by m}=\sbr[2]{\tilde{\textbf{c}}_1\ \tilde{\textbf{c}}_2\ \cdots\ \tilde{\textbf{c}}_m}_{\scrC}.
		\] That is, \[
		\tilde{\textbf{b}}_j=\begin{bmatrix}
			s_{1j} \\ \vdots \\ s_{nj}
		\end{bmatrix}_{\mathscr{B}}=\sum_{i=1}^ns_{ij}\textbf{b}_j\quad\text{and}\quad \tilde{\textbf{c}}_k=\begin{bmatrix}
		t_{1k} \\ \vdots \\ t_{mk}
		\end{bmatrix}_{\mathscr{C}}=\sum_{l=1}^mt_{lk}\textbf{c}_l
		\] for $j=1,\dots,n$ and $k=1,\dots,m$, respectively. We must show that \[
		\textbf{T}\tilde{\textbf{A}_\Phi}=\textbf{A}_\Phi\textbf{S}\in M_{m\by n}(\R).
		\] \begin{enumerate}[(i)]
			\item \((\textbf{T}\tilde{\textbf{A}_\Phi})\) For \(j=1,2,\dots,n\), \[
			\Phi(\tilde{\textbf{b}}_j)=\sum_{k=1}^{m}\tilde{a}_{kj}\tilde{\textbf{c}}_k=\sum_{k=1}^m\sbr{\tilde{a}_{kj}\del{\sum_{l=1}^mt_{lk}\textbf{c}_l}}=\sum_{l=1}^m\sbr{\del{\sum_{k=1}^mt_{lk}\tilde{a}_{kj}}\textbf{c}_l}.
			\]
			\item \((\textbf{A}_\Phi\textbf{S})\) For \(j=1,2,\dots,n\), \[
			\Phi(\tilde{\textbf{b}}_j)=\Phi\of{\sum_{i=1}^ns_{ij}\textbf{b}_j}=\sum_{i=1}^n\sbr{s_{ij}\Phi(\textbf{b}_i)}=\sum_{i=1}^n\sbr{s_{ij}\sum_{i=1}^ma_{li}\textbf{c}_l}=\sum_{l=1}^m\of{\sum_{i=1}^na_{li}s_{ij}}\textbf{c}_l.
			\]
		\end{enumerate}
		 Hence \[
		 \sum_{k=1}^mt_{lk}\tilde{a}_{kj}=\sum_{i=1}^na_{li}s_{ij}\implies\textbf{T}\tilde{\textbf{A}_\Phi}=\textbf{A}_\Phi\textbf{S}\implies\tilde{\textbf{A}}_\Phi=\textbf{T}^{-1}\textbf{A}_\Phi \textbf{S}.
		 \]
	\end{proof}
	
	\begin{example}
		Let \[
		y_1\textbf{e}_1+y_2\textbf{e}_2=\Phi(x_1\textbf{e}_1+x_2\textbf{e}_2)=(x_1+5x_2)\textbf{e}_1+6x_2\textbf{e}_2.
		\] Then \[\begin{bmatrix}
			y_1\\ y_2
		\end{bmatrix}=\textbf{A}_\Phi\begin{bmatrix}
	x_1 \\ x_2
\end{bmatrix},\quad\text{where}\quad
\textbf{A}_\Phi=\begin{bmatrix}
	1 & 5\\ 0 & 6
\end{bmatrix}.
		\] We define \[
		\tilde{\basis}=\sbr{\tilde{\textbf{b}}_1\ \tilde{\textbf{b}}_2}:=\begin{bmatrix}
			1 & 1\\ 1 & 0
		\end{bmatrix}.
		\]\[
		\tilde{\textbf{A}}_\Phi=\textbf{T}^{-1}\textbf{A}_\Phi\textbf{S}=
		\begin{bmatrix}
			0 & 1\\ 1 & -1
		\end{bmatrix}\begin{bmatrix}
		0 & 5\\ 0 & 6
	\end{bmatrix}\begin{bmatrix}
	1 & 1\\ 1 & 0
\end{bmatrix}=\begin{bmatrix}
6 & 0\\ 0 & 1
\end{bmatrix}
		\] \[
		\Phi\of{\tilde{x}_1\begin{bmatrix}
			1 \\ 1
		\end{bmatrix}+\tilde{x}_2\begin{bmatrix}
		1 \\ 0
	\end{bmatrix}}=6\tilde{x}_1\begin{bmatrix}
1 \\ 1
\end{bmatrix}+\tilde{x}_2\begin{bmatrix}
1 \\ 0
\end{bmatrix}.
		\]
	\end{example}

	\begin{tcolorbox}[colframe=defcolor,title={\color{white}\bf Similarity}]
		\begin{definition}
			Let \(\textbf{A},\tilde{\textbf{A}}\in M_{n\by n}(\R)\). \(\textbf{A},\tilde{\textbf{A}}\) are \textbf{similar} if \[
			\exists\textbf{S}\in M_{n\by n}(\R):\tilde{\textbf{A}}=\textbf{S}^{-1}\textbf{A}\textbf{S}.
			\]
		\end{definition}
	\end{tcolorbox}
	
	\newpage
	\subsection{Image and Kernel}
	\begin{tcolorbox}[colframe=defcolor,title={\color{white}\bf Image and Kernel}]
		\begin{definition}
			Let \(\Phi:V\to W\) be a linear mapping. \begin{enumerate}[(1)]
				\item The \textbf{kernel (null) space} is defined by \[
				\ker(\Phi):=\Phi^{-1}(\textbf{0}_W)=\set{\textbf{v}\in V:\Phi(\textbf{v})=\textbf{0}_W}.
				\]
				\item The \textbf{image (range)} is defined by \[
				\img(\Phi):=\Phi[V]=\set{\textbf{w}\in W:(\exists\textbf{v}\in V)\ \Phi(\textbf{v})=\textbf{w}}.
				\]
			\end{enumerate}
		\end{definition}
	\end{tcolorbox}
	\begin{remark}
		\ \begin{enumerate}[(1)]
			\item \(\textbf{0}_V\in\ker(\Phi)\implies\ker\Phi\neq\emptyset\).
			\item \(\ker(\Phi)\subseteq V\) is a subspace of $V$.
			\item \(\img(\Phi)\subseteq W\) is a subspace of $W$.
			\item \(\Phi:V\rightarrowtail W\iff \ker(\Phi)=\set{\textbf{0}_V}\).
		\end{enumerate}
	\end{remark}
	\vspace{8pt}
	\begin{remark}[Null Space and Column Space]
		Let \(\textbf{A}\in M_{m\by n}(\R)\) and $$\fullfunction{\Phi}{\R^n}{\R^m}{\textbf{x}}{\textbf{Ax}}$$
		\begin{enumerate}[(1)]
			\item The \textbf{column space} is the image of \(\Phi\), the span of the columns of \(\textbf{A}\),\begin{align*}
				\img(\Phi)=\set{\textbf{A}\textbf{x}:\textbf{x}\in\R^n}&=\set{\sbr{\textbf{a}_1,\dots,\textbf{a}_n}\begin{bmatrix}
						x_1\\ \vdots\\ x_n
					\end{bmatrix}:x_i\in\R}\\
				&=\set{\sum_{i=1}^nx_i\textbf{a}_i:x_i\in\R}\\
				&=\Span{\textbf{a}_1,\dots,\textbf{a}_n}\subseteq\R^m.
			\end{align*}
			\item \(\rank(\textbf{A})=\dim(\img(\Phi))\).
			\item The \textbf{null space} \(\ker(\Phi)\) is $\set{\textbf{x}:\textbf{Ax}=\textbf{0}}$.
		\end{enumerate}
	\end{remark}

	\newpage
	\begin{example}[Image and Kernel of Linear Mapping]
		The mapping \begin{align*}
			\Phi:\R^4\to\R^2:\begin{bmatrix}
				x_1\\x_2\\x_3\\x_4
			\end{bmatrix}\mapsto\begin{bmatrix}
			1&2&-1&0\\1&0&0&1
		\end{bmatrix}\begin{bmatrix}
			x_1\\x_2\\x_3\\x_4
		\end{bmatrix}&=\begin{bmatrix}
			x_1+2x_2-x_3\\x_1+x_4
		\end{bmatrix}\\
		&=x_1\begin{bmatrix} 1\\1 \end{bmatrix}+x_2\begin{bmatrix} 2\\0 \end{bmatrix}+x_3\begin{bmatrix} -1\\0 \end{bmatrix}+x_4\begin{bmatrix} 0\\1 \end{bmatrix}.
		\end{align*} is linear. Then \begin{enumerate}[(1)]
		\item \(\img(\Phi)=\textnormal{span}\bigg\langle
			\begin{bmatrix}1\\1\end{bmatrix},
			\begin{bmatrix}2\\0\end{bmatrix},
			\begin{bmatrix}-1\\0\end{bmatrix},
			\begin{bmatrix}0\\1\end{bmatrix}
			\bigg\rangle=\R^2
			\)
			\item Since \[
			\begin{bmatrix}
				1&2&-1&0\\1&0&0&1
			\end{bmatrix}\rightsquigarrow\cdots\rightsquigarrow\begin{bmatrix}
			1&0&0&1\\0&1&-\frac{1}{2}&-\frac{1}{2}
		\end{bmatrix}\xrightarrow[]{\text{Minus-1 Trick}}
			\begin{bmatrix}
				1&0&0&1\\0&1&-\frac{1}{2}&-\frac{1}{2}\\
				0&0&-1&0\\ 0&0&0&-1
			\end{bmatrix},
			\] we have \[
			\ker(\Phi)=\Sspan{\begin{bmatrix}1\\-1/2\\-1\\0\end{bmatrix},
					\begin{bmatrix}1\\-1/2\\0\\-1\end{bmatrix}}.
			\]
	\end{enumerate}
	\end{example}
	\vspace{8pt}
	\begin{tcolorbox}[colframe=thmcolor,title={\color{white}\bf Rank-Nullity Theorem (Fundamental Theorem of Linear Mapping)}]
		\begin{theorem}
			Let \(\Phi:V\to W\) be a linear mapping for vector spaces \(V,W\). Then \[
			\dim(\ker\Phi)+\dim(\img\Phi)=\dim V.
			\]
		\end{theorem}
	\end{tcolorbox}

	\section{Affine Spaces}
	\[
	\Phi(\textbf{x})=\textbf{A}\textbf{x}+\textbf{b}
	\] \(\img\Phi\) is not a subspace if \(\textbf{b}\neq 0\).
	
	\newpage
	\chapter{Analytic Geometry}
	
	\section{Norm}
	\begin{tcolorbox}[colframe=defcolor,title={\color{white}\bf Norm}]
		\begin{definition}
			A \textbf{norm} on a vector space \(V\) is a function \[
			\fullfunction{\norm{\ \cdot\ }}{V}{\R}{\textbf{x}}{\norm{\textbf{x}}}
			\] such that for all \(\lambda\in\R\) and \(\textbf{x},\textbf{y}\in V\) the following hold: \begin{enumerate}[(i)]
				\item (Absolutely homogeneous)\quad \(\norm{\lambda x}=\abs{\lambda}\norm{\textbf{x}}\)
				\item (Triangle inequality)\quad \(\norm{\textbf{x}+\textbf{y}}\leq\norm{\textbf{x}}+\norm{\textbf{y}}\)
				\item (Positive definite)\quad \(
				\begin{cases}
					\norm{\textbf{x}}> 0&:\textbf{x}\neq\textbf{0}\\
					\norm{\textbf{x}}=0&:\textbf{x}=\textbf{0}
				\end{cases}\)
			\end{enumerate}
		\end{definition}
	\end{tcolorbox}
	\vspace{8pt}
	\begin{example}[Manhattan Norm]
		The Manhattan norm on $\R^n$ is defined for $\textbf{x}\in\R^n$ as $$\norm{\textbf{x}}_1:=\sum_{i=1}^n\abs{x_i}.$$ The Manhattan norm is also called \(\mathscr{l}_1\) norm.
	\end{example}
	\begin{example}[Euclidean Norm]
		The Manhattan norm on $\R^n$ is defined for $\textbf{x}\in\R^n$ as $$\norm{\textbf{x}}_2:=\sqrt{\sum_{i=1}^nx_i^2}=\sqrt{\textbf{x}^T\textbf{x}}.$$ The Euclidean norm is also called \(\mathscr{l}_2\) norm.
	\end{example}

	\section{Inner Products}
	\subsection{General Inner Product}
	\begin{tcolorbox}[colframe=defcolor,title={\color{white}\bf Dot Product (Scalar Product)}]
		\begin{definition}
			The \textbf{dot product (scalar product)} in \(\R^n\) is given by \[
			\langle\textbf{x},\textbf{y}\rangle=\textbf{x}\cdot\textbf{y}=\textbf{x}^T\textbf{y}=\sum_{i=1}^nx_iy_i.
			\]
		\end{definition}
	\end{tcolorbox}
	\vspace{8pt}
	\begin{tcolorbox}[colframe=defcolor,title={\color{white}\bf Bilinaer Mapping}]
		\begin{definition}
			Let \(V\) be a vector space and \(\Omega:V\times V\to\R\) is a \textbf{bilienar mapping} if for all \(\alpha,\beta\in\R\),  \begin{enumerate}[(i)]
				\item $\Omega(\alpha \textbf{x}_1+\beta\textbf{x}_2,\textbf{y}) = \alpha\Omega(\textbf{x},\textbf{y})+\beta\Omega(\textbf{x}_2,y)$.
				\item $\Omega(\textbf{y},\alpha\textbf{y}_1+\beta\textbf{y}_2) = \alpha\Omega(\textbf{x},\textbf{y}_1)+\beta\Omega(\textbf{x},y_2)$.
			\end{enumerate}
		\end{definition}
	\end{tcolorbox}
	\begin{remark}
		\ \begin{enumerate}[(1)]
			\item \(\Omega\) is called \textbf{symmetric} if \(\forall\textbf{x},\textbf{y}\in V:\Omega(\textbf{x},\textbf{y})=\Omega(\textbf{y},\textbf{x})\).
			\item \(\Omega\) is called \textbf{positive definite} if $
			\begin{cases}
				\Omega(\textbf{x},\textbf{x})>0 &:\textbf{x}\in V\setminus\set{\textbf{0}}\\
				\Omega(\textbf{x},\textbf{x})=0 &:\textbf{x}=\textbf{0}.
			\end{cases}$.
		\end{enumerate}
	\end{remark}
	\vspace{8pt}
	\begin{tcolorbox}[colframe=defcolor,title={\color{white}\bf Inner Product}]
		\begin{definition}
			A positive definite, symmetric bilinear mapping \(\Omega:V\times V\to\R\) is called an \textbf{inner product} on vector space \(V\). 
		\end{definition}
	\end{tcolorbox}
	\begin{example}[Inner Product That Is Not Dot Product]
		Consider \(V=\R^2\). We define \[
		\inner{\textbf{x},\textbf{y}}:=x_1y_1-(x_1y_2+x_2y_1)+2x_2y_2=
		\begin{bmatrix} x_1&x_2\end{bmatrix}
		\begin{bmatrix} 1 & -1\\ -1 &2\end{bmatrix}
		\begin{bmatrix} y_1\\y_2\end{bmatrix}.
		\] Then \begin{enumerate}[(i)]
			\item (positive definite) \[
			\inner{\textbf{x},\textbf{x}}=x_1^2-2x_1x_2+x_2^2+x_2^2=(x_1-x_2)^2+x_2^2\geq0.
			\] Moreover, \(\inner{\textbf{x},\textbf{x}}=0\iff\textbf{x}=0\).
			\item (symmetric) It holds.
			\item (bilinear) It holds.
		\end{enumerate}
	\end{example}
	
	\newpage
	\subsection{Symmetric, Positive Definite Matrices}
	\begin{tcolorbox}[colframe=defcolor,title={\color{white}\bf Symmetric, Positive Defintie Matrix}]
		\begin{definition}
			Let \(V\) be a vector space with \(\dim V=n\). A symmetric matrix \(\textbf{A}\in M_{n\by n}(\R)\) is called \textbf{symmetric, positive definite} if \(\textbf{x}^T\textbf{Ax}\geq 0\) for all \(\textbf{x}\in V\) and \[
			\begin{cases}
				\textbf{x}^T\textbf{Ax}>0 &:\textbf{x}\in V\setminus\set{\textbf{0}}\\
				\textbf{x}^T\textbf{Ax}=0 &:\textbf{x}=\textbf{0}.
			\end{cases}
			\]
		\end{definition}
	\end{tcolorbox}
	\vspace{8pt}
	\begin{tcolorbox}[colframe=thmcolor,title={\color{white}\bf }]
		\begin{theorem}
			Let \(V\) be a vector space with \(\dim V=n\) and \(\basis\) an ordered basis of \(V\). A bilinear mapping \(\inner{\cdot,\cdot}:V\times V\to\R\) is an inner product if and only if \[
			\exists\text{symmetric, positive definite matrix}\ \textbf{A}\in M_{n\by n}(\R):\inner{\textbf{x},\textbf{y}}=\textbf{x}^T\textbf{A}\textbf{y}.
			\]
		\end{theorem}
	\end{tcolorbox} 
	\vspace{8pt}
	\begin{remark}
		Let \(\textbf{A}\) be a symmetric, positive definite matrix.
		\begin{enumerate}[(1)]
			\item \(\ker\textbf{A}=\set{0}\) because \[
			\textbf{x}\neq \textbf{0}\implies \textbf{x}^T\textbf{Ax}>0\implies \textbf{Ax}\neq\textbf{0}.
			\]
			\item The diagonal element \(a_{ii}\) of \(\textbf{A}\) are positive because \[
			a_{ii}=\textbf{e}_i^T\textbf{A}\textbf{e}_i=\inner{\textbf{e}_i,\textbf{e}_i}> 0.
			\]
		\end{enumerate}
	\end{remark}

	\subsection{Lengths and Distances}
	\begin{remark}[Cauchy-Schwarz Inequality]
		\[
		\abs{\inner{\textbf{x},\textbf{y}}}\leq\norms{\textbf{x}}\norms{\textbf{y}}.
		\]
	\end{remark}
	
	\begin{tcolorbox}[colframe=defcolor,title={\color{white}\bf Distance and Metric}]
		\begin{definition}
			Consider an inner product space \((V,\inner{\cdot,\cdot})\). Let \(\textbf{x},\textbf{y}\in V\). Then \[
			d(\textbf{x},\textbf{y})=\norms{\textbf{x}-\textbf{y}}=\sqrt{\inner{\textbf{x}-\textbf{y},\textbf{x}-\textbf{y}}}.
			\] is called \textbf{distance} between \textbf{x} and \textbf{y}. The mapping \[
			\fullfunction{d}{V\times V}{\R}{(\textbf{x},\textbf{y})}{d(\textbf{x},\textbf{y})}
			\] is called a \textbf{metric}
		\end{definition}
	\end{tcolorbox}

	\subsection{Angles and Orthogonality}
	\begin{tcolorbox}[colframe=defcolor,title={\color{white}\bf Angle}]
		\begin{definition}
			Assume that \(\textbf{x},\textbf{y}\in V\setminus\set{\textbf{0}}\). Then \[
			-1\leq\frac{\inner{\textbf{x},\textbf{y}}}{\norms{\textbf{x}}\norms{\textbf{y}}}\leq 1.
			\] And \[
			\exists !\theta\in[0,\pi]:\cos\theta=\frac{\inner{\textbf{x},\textbf{y}}}{\norms{\textbf{x}}\norms{\textbf{y}}}.
			\] The number \(\theta\) is the \textbf{angle}.
		\end{definition}
	\end{tcolorbox}
	\begin{example}
		Consider \(\textbf{x}=(1,1)\) and \(\textbf{y}=(-1,1)\) on \(\R^2\).
		\begin{enumerate}[(1)]
			\item Dot Product: \[
			\textbf{x}\cdot \textbf{y}=(1,1)\cdot(-1,1)=-1+1=0.
			\]
			\item Inner Product: \[
			\inner{\textbf{x},\textbf{y}}:=\textbf{x}^T\begin{bmatrix}
				2&0\\0&1
			\end{bmatrix}\textbf{y}\implies\cos\theta=-\frac{1}{3}.
			\]
		\end{enumerate}
	\end{example}
	\vspace{8pt}
	\begin{tcolorbox}[colframe=defcolor,title={\color{white}\bf Orthogonal Matrix}]
		\begin{definition}
			A square matrix \(\textbf{A}\in M_{n\by n}(\R)\) is an \textbf{orthogonal matrix} if and only if \[
			\textbf{A}\textbf{A}^T=I_n=\textbf{A}^T\textbf{A},
			\] that is, \(\textbf{A}^{-1}=\textbf{A}^T\).
		\end{definition}
	\end{tcolorbox}
	\begin{remark}
		\ \begin{enumerate}[(1)]
			\item \(\textbf{A}^T\textbf{A}=[A_i^TA_j]_{n\by n}=\sbr{\inner{\textbf{A}_i,\textbf{A}_j}}_{n\by n}\), where \(\inner{\textbf{A}_i,\textbf{A}_j}=\begin{cases}
				1 &:i=j\\ 0&:i\neq j.
			\end{cases}\)
			\begin{enumerate}[(i)]
				\item Column vectors of \(\textbf{A}\) are orthogonal each other.
				\item \(\inner{\textbf{A}_i,\textbf{A}_i}=1\implies\norms{\textbf{A}_i}=1\).
			\end{enumerate}
			\item Let \(\textbf{A}\) is orthogonal. Then a linear mapping \[
			\fullfunction{\Phi}{\R^n}{\R^m}{\textbf{x}}{\textbf{Ax}}
			\] has \textbf{length preserving} property, \ie, \(\norms{\textbf{x}}=\norms{\textbf{Ax}}\) because \[
			\norms{\textbf{Ax}}^2=\inner{\textbf{Ax},\textbf{Ax}}=(\textbf{Ax})^T\textbf{Ax}=\textbf{x}^T\textbf{A}^T\textbf{Ax}=\textbf{x}^T\textbf{x}=\inner{\textbf{x},\textbf{x}}=\norms{\textbf{x}}^2.
			\] $\Phi$ has also \textbf{angle preserving} property because \[
			\cos\theta=\frac{(\vec{Ax}^T)(\vec{Ay})}{\norms{\vec{Ax}}\norms{\vec{Ay}}}=\frac{\vec{x}^T\textbf{A}^T\textbf{A}\vec{y}}{\sqrt{\textbf{x}^T\textbf{A}^T\textbf{Ax}\textbf{y}^T\textbf{A}^T\textbf{Ay}}}=\frac{\textbf{x}^T\textbf{y}}{\norms{\vec{x}}\norms{\vec{y}}}.
			\]
		\end{enumerate}
	\end{remark}
	
	\newpage
	\section{Orthonormal Basis}
	\begin{tcolorbox}[colframe=defcolor,title={\color{white}\bf Orthnormal Bais}]
		\begin{definition}
			Consider an \(n\)-dimensional vector space \(V\) and a basis \(\set{\vec{b}_1,\dots,\vec{b}_n}\) of \(V\). The basis is called an \textbf{orthonormal basis (ONB)} if \[
			\inner{\vec{b}_i,\vec{b}_j}=\begin{cases}
				0 &:i\neq j\\
				1 &:i=j
			\end{cases},\quad\ie,\quad\inner{\vec{b}_i,\vec{b}_j}=\delta_{ij}
			\] for all \(i,j=1,\dots,n\).
		\end{definition}
	\end{tcolorbox}
	\vspace{8pt}
	\begin{tcolorbox}[colframe=defcolor,title={\color{white}\bf Orthogonal Complement}]
		\begin{definition}
			Consider a \(d\)-dimensional vector space \(V\) and an \(m\)-dimensional subspace \(U\subseteq V\). The \textbf{orthogonal complement} is \[
			U^\perp:=\set{\textbf{v}\in V:(\forall\vec{u}\in U)\ \inner{\textbf{v},\textbf{u}}=0}
			\]  is a \((d-m)\)-dimensional subspace of \(V\).
		\end{definition}
	\end{tcolorbox}
	\begin{remark}
		\ \begin{enumerate}[(1)]
			\item \(U\cap U^\perp=\set{\vec{0}}\).
			\item Any vector \(\textbf{x}\in V\) can be uniquely decomposed into \[
			\textbf{x}=\sum_{i=1}^m\lambda_m\vec{b}_m+\sum_{j=1}^{d-m}\psi_j\vec{b}_j^\perp,\quad\lambda_i,\psi_j\in\R,
			\] where \((\vec{b}_1,\dots,\vec{b}_m)\) is a basis of \(U\) and \((\vec{b}_1^\perp,\dots,\vec{b}_{d-m}^\perp)\) is a basis of \(U^\perp\).
		\end{enumerate}
	\end{remark}

	\section{Orthogonal Projections}
	\begin{quote}
		``To minimize the compression loss, we have to find the most informative dimensions in the data''
	\end{quote}
	\begin{tcolorbox}[colframe=defcolor,title={\color{white}\bf Projection}]
		\begin{definition}
		Let \(V\) be a vector space and $U\subseteq V$ a subspace of $V$. A linear mapping \(\pi:V\to U\) is called a \textbf{projection} if \[
		\pi^2=\pi\circ\pi=\pi.
		\]
		\end{definition}
	\end{tcolorbox}

	\subsection{Projection onto One-Dimensional Subspaces (Lines)}
	\begin{center}
		\begin{tikzpicture}[scale=2]
			% draw axes
			\draw[->] (-.5,0) -- (3.5,0) node[right] {$\Re(z)$};
			\draw[->] (0,-.5) -- (0,3.5) node[above] {$\Im(z)$};
			% add arrow
			%\draw[->] (0,0) -- (1,1) node[midway, above left] {};
			% draw point
			\draw[thick, ->, dashed, red] (1.5,{1.5*sqrt(3)}) -- ({9/4},{3*sqrt(3)/4}) node[] {};
			\draw[thick, ->, orange] (0,0) -- ({2*sqrt(3)},2) node[right] {$\textbf{b}$};
			\draw[thick, ->] (0,0) -- (1.5,{1.5*sqrt(3)}) node[right] {$\textbf{x}$};
			\draw[->, thick] ({sqrt(3)/2},1/2) arc (30:60:1) node[midway, right] {$\omega$};
			\fill ({9/4},{3*sqrt(3)/4}) circle (1.5pt) node[below] {$\pi_U(\vec{x})$};
		\end{tikzpicture}
	\end{center}
	We determine the coordinate \(\lambda\), the projection \(\pi_U(\vec{x})\in U\), and the projection matrix \(\textbf{P}_\pi\) that maps any \(\vec{x}\in\R^n\) onto $U$:
	\begin{enumerate}[(Step 1)]
		\item Finding the coordinate \(\lambda\). \(\pi_U\in U\Rightarrow\pi_U(\vec{x})=\lambda\vec{b}\). Note that \begin{align*}
			0&=\inner{\vec{x}-\pi_U(\vec{x}),\vec{b}}\\
			&=\inner{\vec{x}-\lambda\vec{b},\vec{b}}\quad\because\pi_U(\vec{x})=\lambda\vec{b}\\
			&=\inner{\vec{x},\vec{b}}-\lambda\inner{\vec{b},\vec{b}}\quad\text{by bilinearity of the inner product}.
		\end{align*} Thus \[
		\lambda=\frac{\inner{\vec{x},\vec{b}}}{\inner{\vec{b},\vec{b}}}=\frac{\inner{\vec{b},\vec{x}}}{\norms{\vec{b}}^2}=\frac{\vec{b}^T\vec{x}}{\vec{b}^T\vec{b}}.
		\] If \(\norms{\vec{b}}=1\), then the coordinate $\lambda$ of the projection is given by \(\vec{b}^T\vec{x}\).
		\item Finding the projection point \(\pi_U(\vec{x})\in U\) and the projection matrix \(\textbf{P}_\pi\).
		Note that \[
		\inner{\vec{b},\vec{x}}\vec{b}=\del{\vec{b}^T\vec{x}}\vec{b}=\del{\sum_jb_jx_j}\del{\sum_ib_i\vec{e}_i}=\sum_i\del{\sum_j b_ib_jx_j}\vec{e}_i=\sum_{ij} (\vec{b}\vec{b}^T)_{ij}x_i\vec{e}_i=\vec{b}\vec{b}^T\vec{x}
		\]
		\[
		\pi_U(\vec{x})=\lambda\vec{b}=\underbrace{\del{\frac{\inner{\vec{x},\vec{b}}}{\norms{\vec{b}}^2}}}_{\in\R}\vec{b}=\textbf{P}_\pi\vec{x},\quad\text{where}\quad\textbf{P}_\pi=\del{\frac{\vec{b}\vec{b}^T}{\norms{\vec{b}}^2}}.
		\]
	\end{enumerate}
	\vspace{4pt}
	\begin{example}[Projection onto a Line]
		Find the projection matrix \(\textbf{P}_\pi\) onto the line through the origin spanned by \(\vec{b}=\begin{bmatrix}
			1&2&2
		\end{bmatrix}^T\), where \(\vec{b}\) is a direction and a basis of the one-dimensional subspace (line through origin).
		\begin{proof}[\sol]
			Note that
			\begin{align*}
				&\vec{b}\vec{b}^T=\begin{bmatrix}
					1\\2\\2
				\end{bmatrix}\begin{bmatrix}
				1&2&2
			\end{bmatrix}=\begin{bmatrix}
			1&2&2\\2&4&4\\2&4&4
		\end{bmatrix},\\
			&\norms{\vec{b}}^2=\vec{b}^T\vec{b}=\begin{bmatrix}
				1&2&2
			\end{bmatrix}\begin{bmatrix}
				1\\2\\2
			\end{bmatrix}=1+2^2+2^2=9.
			\end{align*} Thus \[
			\textbf{P}_{\pi}=\frac{\vec{b}\vec{b}^T}{\vec{b}^T\vec{b}}=\frac{1}{9}
				\begin{bmatrix}
				1&2&2\\2&4&4\\2&4&4
			\end{bmatrix}.
			\] For \(\textbf{x}=\begin{bmatrix}
				1&1&1
			\end{bmatrix}^T\in \R^3\), the projection is \[
			\pi_U(\vec{x})=\textbf{P}_\pi\vec{x}=\frac{1}{9}\begin{bmatrix}
				1&2&2\\2&4&4\\2&4&4
			\end{bmatrix}\begin{bmatrix}
			1 \\ 1 \\ 1
		\end{bmatrix}=\frac{1}{9}\begin{bmatrix}
		5\\10\\10
		\end{bmatrix}\in\textnormal{span}\bigg\langle\begin{bmatrix}
		1\\2\\2
	\end{bmatrix}\bigg\rangle.
			\]
		\end{proof}
	\end{example}

	\subsection{Projection onto General Subspaces}
	
	Assume that \[
	U=\Span{\vec{b}_1,\dots,\vec{b}_m}\subseteq V=R^n.
	\] Then \(\pi_U(\vec{x})=\sum_{i=1}^m\lambda_i\vec{b}_i\).
	
	We find the projection \(\pi_U(\vec{x})\) and the projection matrix \(\textbf{P}_\pi\):
	
	\begin{enumerate}[(Step 1)]
		\item Find the coordinates \(\lambda_1,\dots,\lambda_m\) of projection w.r.t. the basis of \(U\), such that the linear combination \begin{align*}
			&\pi_U(\vec{x})=\sum_{i=1}^m\lambda_i\vec{b}_i=\textbf{B}\boldsymbol{\lambda}\quad\text{with}\\
			&\textbf{B}=\begin{bmatrix}
				\vec{b}_1,\dots,\vec{b}_m
			\end{bmatrix}\in M_{n\by m}(\R),\quad\boldsymbol{\lambda}=\begin{bmatrix}
			\lambda_1,\dots,\lambda_m^T
		\end{bmatrix}\in\R^m
		\end{align*} is closest to \(\vec{x}\in\R^n\).
		We obtain \(m\) simulationeous conditions \begin{align*}
			\inner{\vec{b}_1,\vec{x}-\pi_U(\vec{x})}=&\vec{b}_1^T(\vec{x}-\pi_U(\vec{x}))=0\\
			&\vdots\\
			\inner{\vec{b}_m,\vec{x}-\pi_U(\vec{x})}=&\vec{b}_m^T(\vec{x}-\pi_U(\vec{x}))=0
		\end{align*} which, with \(\pi_U(\vec{x})=\textbf{B}\boldsymbol{\lambda}\), can be written as \begin{align*}
		\vec{b}_1^T(&\vec{x}-\textbf{B}\boldsymbol{\lambda})=0\\
		&\vdots\\
		\vec{b}_m^T(&\vec{x}-\textbf{B}\boldsymbol{\lambda})=0\\
	\end{align*} such that we obtain a homogeneous linear equation system \begin{align*}
		\begin{bmatrix}
			\vec{b}_1^T\\ \vdots\\ \vec{b}_m^T
		\end{bmatrix}\begin{bmatrix}
		\vec{x}-\textbf{B}\boldsymbol{\lambda}
	\end{bmatrix}=\vec{0}&\iff\textbf{B}^T(\vec{x}-\textbf{B}\boldsymbol{\lambda})=0\\
	&\iff\textbf{B}^T\textbf{B}\boldsymbol{\lambda}=\textbf{B}^T\vec{x}.
		\end{align*} Thus the coordinate (coefficient) is\[
		\boldsymbol{\lambda}=(\textbf{B}^T\textbf{B})^{-1}\textbf{B}^T\vec{x}.
		\]
		\item Find the projection \(\pi_U(\vec{x})\in U\). \[
		\pi_U(\vec{x})=\textbf{B}\boldsymbol{\lambda}=\textbf{B}(\textbf{B}^T\textbf{B})^{-1}\textbf{B}^T\vec{x}.
		\]
		\item Find the projection \(\textbf{P}_\pi\). \[
		\textbf{P}_\pi=\textbf{B}(\textbf{B}^T\textbf{B})^{-1}\textbf{B}^T.
		\]
	\end{enumerate}
	\vspace{4pt}
	\begin{example}[Projection onto a Two-dimensional Subspace]
		For a subspace \[U=\textnormal{span}\bigg\langle\begin{bmatrix}
			1\\1\\1
		\end{bmatrix}\begin{bmatrix}
		0\\1\\2
	\end{bmatrix}\bigg\rangle\subseteq\R^3\quad\text{and}\quad\vec{x}=\begin{bmatrix}
	6\\0\\0
\end{bmatrix}\in\R^3,\] find the coordinates \(\lambda\) of \(\vec{x}\) in terms of the subspace \(U\), the projection point \(\pi_U(\vec{x})\) and the projection matrix \(\textbf{P}_\pi\).
	\begin{proof}[\sol]
		\[
		\vec{x}=\begin{bmatrix}
			6\\0\\0
		\end{bmatrix}\implies\textbf{P}_\pi\vec{x}=5\begin{bmatrix}
		1\\1\\1
	\end{bmatrix}+(-3)\begin{bmatrix}
	0\\1\\2
\end{bmatrix}.
		\]
	\end{proof}
	\end{example}

	\subsection{Gram-Shmidt Orthogonalization}
	The \textit{Gram-Schmidt orthogonalization} method iteratively constructs an orthogonal basis \((\textbf{u}_1,\dots,\textbf{u}_n)\) from any basis \((\vec{b}_1,\dots,\vec{b}_n)\) of \(V\) as follows: \begin{align*}
		\vec{u}_1&:=\vec{b}_1\\
		\vec{u}_2&:=\vec{b}_2-\pi_{\Span{\vec{u}_1}}(\vec{b}_2)\\
		&\vdots\\
		\vec{u}_k&:=\vec{b}_k-\pi_{\Span{\vec{u}_1,\dots,\vec{u}_{k-1}}}(\vec{b}_k),\quad k=2,\dots, n.
	\end{align*}
	If we normalize \(\vec{u}_k\) at each step, that is \[
	\hat{\vec{u}}_k:=\frac{\vec{u}_k}{\norms{\vec{u}_k}},
	\] we obtain an orthonormal basis.
	
	\newpage
	\chapter{Matrix Decompositions}
	\section{Determinant and Trace}
	\begin{tcolorbox}[colframe=defcolor,title={\color{white}\bf Determinant }]
		\begin{definition}
			The \textbf{determinant} of a square matrix \(\textbf{A}\in M_{n\by n}(\R)\) is a function \[
			\fullfunction{\det}{M_{n\by n}}{\R}{A}{\det(A)}.
			\]
		\end{definition}
	\end{tcolorbox}
	\begin{remark}
		 \ \begin{enumerate}[(1)]
			\item ($n=2$) \[
			\textbf{A}=\begin{bmatrix}
				a&b\\c&d
			\end{bmatrix}\implies\det\textbf{A}=ad-bc.
			\]
			\item ($n=3$) \begin{align*}
				\det\textbf{A}=\begin{vmatrix}
					a_{11}&a_{12}&a_{13}\\
					a_{21}&a_{22}&a_{23}\\
					a_{31}&a_{32}&a_{33}
				\end{vmatrix}=&a_{11}\begin{vmatrix}
				a_{22}&a_{23}\\a_{32}&a_{33}
			\end{vmatrix}-a_{12}\begin{vmatrix}
			a_{21}&a_{23}\\a_{31}&a_{33}
		\end{vmatrix}+a_{13}\begin{vmatrix}
		a_{21}&a_{22}\\a_{31}&a_{32}
	\end{vmatrix}\\
			=&a_{11}(a_{22}a_{33}-a_{23}a_{32})
			-a_{12}(a_{21}a_{33}-a_{23}a_{31})\\
			&+a_{13}(a_{21}a_{32}-a_{22}a_{31}).
			\end{align*}
		\end{enumerate}
	\end{remark}
	\vspace{4pt}
	\begin{tcolorbox}[colframe=thmcolor,title={\color{white}\bf }]
		\begin{theorem}
			Let \(\textbf{A}\in M_{n\by n}(\R)\). $
			\exists\textbf{A}^{-1}\iff\det(\textbf{A})\neq 0.
			$	
		\end{theorem}
	\end{tcolorbox}
	\vspace{4pt}
	\begin{tcolorbox}[colframe=defcolor,title={\color{white}\bf Upper and Lower Triangluar Matrix}]
		\begin{definition}
			\ \begin{enumerate}[(1)]
				\item \textbf{U} is an upper triangular matrix if \(u_{ij}=0\) for \(i>j\).
				\item \textbf{L} is an lower triangular matrix if \(l_{ij}=0\) for \(i<j\).
			\end{enumerate}	
		\end{definition}
	\end{tcolorbox}
	\begin{remark}
		Note that \(\det\textbf{U}=\sum_{i=1}^n u_{ii}\) and \(\det\textbf{L}=\sum_{i=1}^nl_{ii}\).
	\end{remark}
	\newpage
	\begin{tcolorbox}[colframe=procolor,title={\color{white}\bf }]
		\begin{proposition}
			\ \begin{enumerate}[(1)]
				\item \(\det(\textbf{A}\textbf{B})=\det(\textbf{A})\det(\textbf{B})\)
				\item \(\det(\textbf{A})=\det(\textbf{A}^T)\)
				\item \(\det(\textbf{A}^{-1})=\sbr{\det(\textbf{A})}^{-1}\)
				\item \(\textbf{B}=\textbf{S}^{-1}\textbf{A}\textbf{S}\implies\det(\textbf{A})=\det(\textbf{B})\)
				\item \(\det(\lambda\textbf{A})=\lambda^n\det(\textbf{A})\) for \(\textbf{A}\in M_{n\by n}(\R)\)
			\end{enumerate}
		\end{proposition}
	\end{tcolorbox}
	\vspace{8pt}
	\begin{tcolorbox}[colframe=thmcolor,title={\color{white}\bf }]
		\begin{theorem}
			Let \(\textbf{A}\in M_{n\by n}(\R)\). Then \[
			\det(\textbf{A})\neq 0\iff\rank(\textbf{A})=n.
			\] In other words, \(\textbf{A}\) is invertible if and only if it is full rank.
		\end{theorem}
	\end{tcolorbox}
	\vspace{8pt}
	\begin{tcolorbox}[colframe=defcolor,title={\color{white}\bf Trace}]
		\begin{definition}
			The \textbf{trace} of a square matrix \(\textbf{A}\in M_{n\by n}(\R)\) is defined as \[
			\tr(\textbf{A}):=\sum_{i=1}^na_{ii},
			\] \ie, the trace is the sum of the diagonal elements of \(\textbf{A}\).
		\end{definition}
	\end{tcolorbox}
	\vspace{4pt}
	\begin{tcolorbox}[colframe=procolor,title={\color{white}\bf }]
		\begin{proposition}
			\ \begin{enumerate}[(1)]
				\item \(\tr(\textbf{A}+\textbf{B})=\tr(\textbf{A})+\tr(\textbf{B})\) for \(\textbf{A},\textbf{B}\in M_{n\by n}(\R)\)
				\item \(\tr(\alpha\textbf{A})=\alpha\tr(\textbf{A})\) for \(\alpha\in\R,\textbf{A}\in M_{n\by n}(\R)\)
				\item \(\tr(\textbf{I}_n)=n\)
				\item \(\tr(\textbf{A}\textbf{B})=\tr(\textbf{B}\textbf{A})\) for \(\textbf{A}\in M_{n\by k}(\R),\textbf{B}\in M_{k\by n}(\R)\)
			\end{enumerate}
		\end{proposition}
	\end{tcolorbox}
	\begin{proof}
		(4) Let \(\textbf{A}=[a_{ij}]_{n\by k}\) and \(\textbf{B}=[b_{ij}]_{k\by n}\), and let
		\begin{align*}
			\textbf{AB}:=\textbf{C}=[c_{ij}]_{n\by n}\quad\text{with}\quad c_{ij}=\sum_{l=1}^na_{il}b_{lj},\\
			\textbf{BA}:=\textbf{D}=[d_{ij}]_{k\by k}\quad\text{with}\quad d_{ij}=\sum_{l=1}^kb_{il}a_{lj}.
		\end{align*}
		Then \begin{align*}
			\tr(\textbf{A}\textbf{B})=\sum_{l=1}^{m}c_{ll}
		\end{align*}
	\end{proof}
	
	\begin{tcolorbox}[colframe=defcolor,title={\color{white}\bf Charateristic Polynomial}]
		\begin{definition}
			Let \(\lambda\in\R\) and \(\textbf{A}\in M_{n\by n}(\R)\). Then \[
			p_\textbf{A}(\lambda):=\det(\textbf{A}-\lambda\textbf{I}_n)=\sum_{i=0}^nc_i\lambda^n\quad\text{with}\quad c_i=\quad\begin{cases}
				\det(\textbf{A}) &:i=0\\
				(-1)^n\tr(\textbf{A})&:i\in(0,n)\\
				(-1)^n &:i=n
			\end{cases}
			\] is the \textbf{characteristic polynomial} of \(\textbf{A}\).
		\end{definition}
	\end{tcolorbox}
	
	\newpage
	\section{Eigenvalues and Eigenvectors}
	\subsubsection{Eigenvalues and Eigenvectors}
	\begin{tcolorbox}[colframe=defcolor,title={\color{white}\bf Eigenvalue and Eigenvetor}]
		\begin{definition}
			Let \(\textbf{A}\in M_{n\by n}(\R)\). Then \(\lambda\in\R\) is an \textbf{eigenvalue} of \(\textbf{A}\) and \(\textbf{v}\in\R^n\setminus\set{\textbf{0}}\) is the corresponding eigenvector of \(\textbf{A}\) if \[
			\textbf{Av}=\lambda\textbf{v}.
			\]
		\end{definition}
	\end{tcolorbox}
	\vspace{4pt}
	\begin{tcolorbox}[colframe=thmcolor,title={\color{white}\bf }]
		\begin{theorem}
			TFAE(The following are equivalent):
			\begin{enumerate}[(1)]
				\item \(\lambda\) is an eigenvalue of \(\textbf{A}\in M_{n\by n}(\R)\).
				\item \(\exists\vec{v}\in\R^n\setminus\set{\textnormal{\bf 0}}:\textbf{Av}=\lambda\vec{v}\).
				\item \(\rank(\textbf{A}-\lambda\textbf{I}_n)<n\).
				\item \(\det(\textbf{A}-\lambda\textbf{I}_n)=0\).
			\end{enumerate}
		\end{theorem}
	\end{tcolorbox}
	\vspace{4pt}
	\begin{tcolorbox}[colframe=thmcolor,title={\color{white}\bf }]
		\begin{theorem}
			\(\lambda\in\R\) is an eigenvalue of \(\textbf{A}\) \(\iff\) \(\lambda\) is a root of the characteristic polynomial \(p_\textbf{A}(\lambda)\) of \(\textbf{A}\).
		\end{theorem}
	\end{tcolorbox}
	\vspace{4pt}
	\begin{example}[Computing Eigenvalue, Eigenvectors, and Eigenspaces]
		Find the eigenvalues and eigenvectors of the \(2\by 2\) matrix \[
		\textbf{A}=\begin{bmatrix}
			4&2\\1&3
		\end{bmatrix}.
		\]
		\begin{proof}[\sol]
			\begin{enumerate}[\bf (Step 1)]
				\item \textbf{Characteristic Polynomial and Eigenvalues.} 
				\begin{align*}
					p_\textbf{A}(\lambda)=\det(\textbf{A}-\lambda \textbf{I}_2)=\begin{vmatrix}
						4-\lambda&2\\13-\lambda
					\end{vmatrix}
				=(4-\lambda)(3-\lambda)-2&=\lambda^2-7\lambda+10\\
				&=(\lambda-2)(\lambda-5).
				\end{align*} Thus, we obtain roots \(\lambda_1=2\) and \(\lambda_2=5\).
				\item \textbf{Eigenvalues and Eigenspaces.} We solve $
				\begin{bmatrix}
					4-\lambda&2\\1&3-\lambda
				\end{bmatrix}\textbf{x}=\textbf{0}.
				$ \begin{enumerate}[(i)]
					\item (\(\lambda_1=2\))\begin{align*}
						\begin{bmatrix}
							2&2\\1&1
						\end{bmatrix}\begin{bmatrix}
						x_1\\x_2
					\end{bmatrix}=\begin{bmatrix}
					0\\0
				\end{bmatrix}\implies\begin{bmatrix}
				x_1\\x_2
			\end{bmatrix}=\begin{bmatrix}
			1\\-1
		\end{bmatrix}\implies C(\lambda_1)=\textnormal{span}\bigg\langle\begin{bmatrix}
		1\\-1
	\end{bmatrix}\bigg\rangle.
					\end{align*}
				\item (\(\lambda_1=5\))\begin{align*}
					\begin{bmatrix}
						-1&2\\1&-2
					\end{bmatrix}\begin{bmatrix}
						x_1\\x_2
					\end{bmatrix}=\begin{bmatrix}
						0\\0
					\end{bmatrix}\implies\begin{bmatrix}
						x_1\\x_2
					\end{bmatrix}=\begin{bmatrix}
						2\\1
					\end{bmatrix}\implies C(\lambda_2)=\textnormal{span}\bigg\langle\begin{bmatrix}
					2\\1
				\end{bmatrix}\bigg\rangle.
				\end{align*}
				\end{enumerate}
			\end{enumerate}
		\end{proof}
	\end{example}

	\begin{tcolorbox}[colframe=defcolor,title={\color{white}\bf Defective}]
		\begin{definition}
			A square matrix \(\textbf{A}\in M_{n\by n}(\R)\) is \textbf{defective} if it possesses fewer than \(n\) linearly independent eigenvectors.
		\end{definition}
	\end{tcolorbox}
	\begin{remark}
		\ \begin{enumerate}[(1)]
			\item \(\textbf{A}\) has \(n\) distinct eigenvalue \(\Rightarrow\) \textbf{A} is not defective.
			\item For a defective matrix \(\textbf{A}\in M_{n\by n}(\R)\), the sum of the dimension of eigenspaces \(<n\).
			\item A defective matrix has at lest one eigenvalue \(\lambda_i\) with an algebraic multiplicity \(m>1\) and a geometric multiplicity of less than \(m\). Note that \[
			\text{``Algebraic Multiplicity''}\geq\text{``Geometric Multiplicity''}
			\]
			\item \(\textbf{A}\) is defective iff \(\sum_i\dim C(\lambda_i)\neq n\).
		\end{enumerate}
	\end{remark}
	\vspace{4pt}
	\begin{tcolorbox}[colframe=thmcolor,title={\color{white}\bf }]
		\begin{theorem}
			\ \begin{enumerate}[(1)]
				\item $\textbf{A}$, $\textbf{A}^T$ have the same eigenvalues.
				\item Similar matrices have the same eigenvalues.
				\item \hl{Symmetric, positive definite matrices always have positive real eigenvalues}.
			\end{enumerate}
		\end{theorem}
	\end{tcolorbox}
	\begin{proof}
		\begin{enumerate}[(1)]
			\item Since \((\textbf{A}-\lambda I)^T=\textbf{A}^T-\lambda I\) and \(\det(\textbf{A})=\det(\textbf{A}^T)\), \[
			\det(\textbf{A}^T-\lambda \textbf{I})=\det((\textbf{A}-\lambda \textbf{I})^T)=\det(\textbf{A}-\lambda\textbf{I}).
			\]
			\item Let \(\hat{\textbf{A}}=\textbf{S}^{-1}\textbf{A}\textbf{S}\). Since \[
			\hat{\textbf{A}}-\lambda\textbf{I}=\textbf{S}^{-1}\textbf{A}\textbf{S}-\textbf{S}^{-1}\lambda\textbf{I}\textbf{S}=\textbf{S}^{-1}[\textbf{A}-\lambda\textbf{I}]\textbf{S},
			\] we have \[
			\det(\hat{\textbf{A}}-\lambda\textbf{I})=\det(\textbf{S}^{-1}[\textbf{A}-\lambda\textbf{I}]\textbf{S})=\det(\textbf{S}^{-1})\det(\textbf{A}-\lambda\textbf{I})\det(\textbf{S})=\det(\textbf{A}-\lambda\textbf{I}).
			\]
			\item Let \(\textbf{A}\) is symmetric, positive definite matrix. Let \(\textbf{A}\textbf{x}=\lambda\vec{x}\). Then \[
			\textbf{x}^TA\textbf{x}=\textbf{x}^T\lambda\vec{x}=\lambda\norms{\vec{x}}\geq 0.
			\] Since \(\vec{x}\neq 0\implies\norms{\vec{x}}>0\land\vec{x}^T\textbf{A}\vec{x}>0\), we have \(\lambda>0\).
		\end{enumerate}
	\end{proof}
	\vspace{4pt}
	\begin{example}[Defective Matrix]
		Let \[
		\textbf{A}=\begin{bmatrix}
			2&1&0\\0&2&0\\0&0&3
		\end{bmatrix}.
		\] Then \[
		\det(\textbf{A}-\lambda\textbf{I})=\begin{vmatrix}
			2-\lambda&1&0\\0&2-\lambda&0\\0&0&3-\lambda
		\end{vmatrix}=(3-\lambda)(2-\lambda)^2=0\implies\begin{cases}
		\lambda_1=3\\
		\lambda_2=2.
	\end{cases}
		\] And so \begin{align*}
			&(\textbf{A}-\lambda_1\textbf{I})\textbf{x}_1=0\iff\begin{bmatrix}
				-1&1&0\\0&-1&0\\0&0&0
			\end{bmatrix}\textbf{x}_1=\textbf{0}&\implies\textbf{x}_1=\begin{bmatrix}
			0\\0\\1
		\end{bmatrix},\\
		&(\textbf{A}-\lambda_2\textbf{I})\textbf{x}_2=0\iff\begin{bmatrix}
			0&1&0\\0&0&0\\0&0&1
		\end{bmatrix}\textbf{x}_1=\textbf{0}&\implies\textbf{x}_1=\begin{bmatrix}
			1\\0\\0
		\end{bmatrix}.
		\end{align*}
	\end{example}
	\vspace{4pt}
	\begin{example}
		Let \[
		\textbf{A}=\begin{bmatrix}
			0&1&\\-1&0
		\end{bmatrix}\in M_{2\by 2}(\R).
		\] Then \[
		\det(\textbf{A}-\lambda\textbf{I})=\begin{vmatrix}
			-\lambda&1\\-1&-\lambda
		\end{vmatrix}=\lambda^2+1=0.
		\] And so \begin{align*}
			(\textbf{A}-\lambda_1\textbf{I})\textbf{x}_1=\textbf{0}&\iff\begin{bmatrix}
				-i&1\\-1&-i
			\end{bmatrix}\textbf{x}_1=\textbf{0}&\implies\textbf{x}_1=\begin{bmatrix}
			1\\i
		\end{bmatrix},\\
		(\textbf{A}-\lambda_2\textbf{I})\textbf{x}_2=\textbf{0}&\iff\begin{bmatrix}
		i&1\\-1&i
	\end{bmatrix}\textbf{x}_1=\textbf{0}&\implies\textbf{x}_1=\begin{bmatrix}
	1\\-i
\end{bmatrix}.
		\end{align*}
	\end{example}
	\vspace{4pt}
	\begin{example}
		Let \[
		\textbf{A}=\begin{bmatrix}
			2&3-3i&\\3+3i&5
		\end{bmatrix}\in M_{2\by 2}(\C).
		\] Then \[
		\det(\textbf{A}-\lambda\textbf{I})=\begin{vmatrix}
			2-\lambda&3-3i\\3+3i&5-\lambda
		\end{vmatrix}=\lambda^2-7\lambda-8=(\lambda+1)(\lambda-8)=0\implies\begin{cases}
		\lambda_1=8\\ \lambda_2=-1.
	\end{cases}
		\] And so \begin{align*}
			(\textbf{A}-\lambda_1\textbf{I})\textbf{x}_1=\textbf{0}&\iff\begin{bmatrix}
				-6&3-3i\\3+3i&-3
			\end{bmatrix}\textbf{x}_1=\textbf{0}&\implies\textbf{x}_1=\begin{bmatrix}
				1\\1+i
			\end{bmatrix},\\
			(\textbf{A}-\lambda_2\textbf{I})\textbf{x}_2=\textbf{0}&\iff\begin{bmatrix}
				3&3-3i\\3+3i&6
			\end{bmatrix}\textbf{x}_1=\textbf{0}&\implies\textbf{x}_1=\begin{bmatrix}
				1-i\\-i
			\end{bmatrix}.
		\end{align*}
	\end{example}

	\subsubsection{Complex Matrices}
	Consider complex vector \[
	\vec{w}=(w_1,\dots,w_n)\in\C^n\quad\text{with}\quad w_j=x_j+iy_j,
	\] where \(x_j,y_j\in\R\). Then \begin{enumerate}[(1)]
		\item Norm: \[
		\norms{\vec{w}}^2=\sum_{j=1}^n\abs{w_j}^2\quad\text{with}\quad \abs{w_j}=\sqrt{x_j^2+y_j^2}.
		\]
		\item Inner Product: For \(\vec{w},\vec{z}\in\C^n\), \[		
		\inner{\vec{w},\vec{z}}:=\conjugate{\vec{w}^T}\vec{z}=\sum_{j=1}^n\conjugate{w}_jz_j.
		\] Note that \(\inner{\vec{z},\vec{z}}=\sum_{j=1}^n\conjugate{z}_jz_j=\sum_{j=1}^n\abs{z_j}^2=\norms{\vec{z}}^2\).
	\end{enumerate} 
	\vspace{8pt}
	\begin{tcolorbox}[colframe=defcolor,title={\color{white}\bf Hermition}]
		\begin{definition}
			Let \(\textbf{A}\in M_{n\by n}(\C)\). Then \[
			\textbf{A}^H:\conjugate{\textbf{A}}^T.
			\] is called \textbf{Hermition} of \(\textbf{A}\).
		\end{definition}
	\end{tcolorbox}
	\begin{example}
		\[
		\textbf{A}=\begin{bmatrix}
			1&1+i\\1-i&i
		\end{bmatrix}\implies\conjugate{\textbf{A}}=\begin{bmatrix}
		1&1-i\\1+i&-i
	\end{bmatrix}\implies\textbf{A}^H=\conjugate{\textbf{A}}^T=\begin{bmatrix}
	1&1+i\\1-i&-i
\end{bmatrix}.
		\]
	\end{example}
	\vspace{4pt}
	\begin{tcolorbox}[colframe=defcolor,title={\color{white}\bf Hermitian Matrix}]
		\begin{definition}
			\(\textbf{A}\) is a \textbf{Hermitian matrix} if \(\textbf{A}=\textbf{A}^H\).
		\end{definition}
	\end{tcolorbox}
	\begin{remark}
		\ \begin{enumerate}[(1)]
			\item A real symmetric matrix \(\textbf{A}\) is a Hermitian matrix.
			\item A Hermitian matrix has real eigenvalues.
		\end{enumerate}
	\end{remark}
	\newpage
	\begin{tcolorbox}[colframe=thmcolor,title={\color{white}\bf H1}]
		\begin{theorem}
			\(\textbf{A}=\textbf{A}^H\implies (\forall\vec{x}\in\C^n)\  \vec{x}^H\textbf{A}\vec{x}\in\R\).
		\end{theorem}
	\end{tcolorbox}
	\begin{proof}
		Suppose that \(\textbf{A}=\textbf{A}^H\). Let \(\textbf{y}:=\textbf{x}^H\textbf{A}\textbf{x}\). We must show that \[
		\textbf{y}=\textbf{y}^H,\quad\ie,\quad \textbf{y}=\conjugate{\textbf{y}}\ (\Rightarrow\textbf{y}\in\R).
		\] \[
		\textbf{y}^H=\del{\vec{x}^H\textbf{A}\vec{x}}^H=\vec{x}^H\textbf{A}^H(\vec{x}^H)^H=\textbf{x}^H\textbf{A}\vec{x}=\vec{y}.
		\]
	\end{proof}
	\vspace{4pt}
	\begin{tcolorbox}[colframe=thmcolor,title={\color{white}\bf H2}]
		\begin{theorem}
			If \textbf{A} is Hermitian, then every eigenvalue is real.
		\end{theorem}
	\end{tcolorbox}
	\begin{proof}
		Let \(\textbf{A}\vec{v}=\lambda\vec{v}\) with $\vec{v}\neq\textbf{0}$. By Theorem H1, \[
		\vec{v}^H\textbf{A}\vec{v}=\vec{v}^H(\lambda\vec{v})=\lambda\vec{v}^H\vec{v}=\lambda\norms{\vec{v}}^2\implies\lambda=\frac{\vec{v}^H\textbf{A}\vec{v}}{\norms{\vec{v}}^2}\in\R.
		\]
	\end{proof}
	\vspace{4pt}
	\begin{tcolorbox}[colframe=thmcolor,title={\color{white}\bf H3}]
		\begin{theorem}
			If \(\textbf{A}\in M_{n\by n}(\C)\) is Hermitian, then two eigenvectors corresponding to different eigenvalues are orthogonal.
		\end{theorem}
	\end{tcolorbox}
	\begin{proof}
		For a Hermitian matrix \(\textbf{A}\), let \[
		\textbf{A}\vec{v}_1=\lambda_1\vec{v}_1,\quad\textbf{A}\vec{v}_2=\lambda_2\vec{v}_2
		\] with \(\lambda_1\neq\lambda_2\). Then 
		\begin{align*}
			&\vec{v}_1\textbf{A}\vec{v}_2=\vec{v}_1^H\lambda_2\vec{v}_2=\lambda_2\vec{v}_1^H\vec{v}_2,\\
			&\vec{v}_1\textbf{A}\vec{v}_2=\vec{v}_1^H\textbf{A}^H\vec{v}_2=(\textbf{A}\vec{v}_1)^H\vec{v}_2=(\lambda_1\vec{v}_1)^H\vec{v}_2=\lambda_1\vec{v}_1^H\vec{v}_2.
		\end{align*} Thus, 
		\begin{align*}
			&\lambda_1\vec{v}_1^H\vec{v}_2=\lambda_2\vec{v}_1^H\vec{v}_2\\
			\iff&\lambda_1\inner{\vec{v}_1,\vec{v}_2}-\lambda_2\inner{\vec{v}_1,\vec{v}_2}=0\\
			\iff&(\lambda_1-\lambda_2)\inner{\vec{v}_1,\vec{v}_2}=0\\
			\iff&\inner{\vec{v}_1,\vec{v}_2}=0\quad\because\lambda_1\neq\lambda_2\\
			\iff&\vec{v}_1\perp\vec{v}_2.
		\end{align*}
	\end{proof}
	\newpage
	\subsubsection{Spectral Theorem}
	\begin{tcolorbox}[colframe=thmcolor,title={\color{white}\bf Spectral Theorem}]
		\begin{theorem}
			Let \(\textbf{A}\in M_{n\by n}(\R)\) is symmetric. Then \[
			\exists\text{orthonormal basis of the corresponding vector space $V$ consisting of}
			\] eigenvalues of \(\textbf{A}\), and each eigenvalue is real.
		\end{theorem}
	\end{tcolorbox}
	\begin{proof}
	By Theorem H1, every eigenvalue is real. We remain to show that eigenvalues generate orthonormal basis.
	\begin{enumerate}[(i)]
		\item All eigenvalues are distinct, say, \(\lambda_1\neq\lambda_2\neq\cdots\neq\lambda_n\). By Theorem H3, \[
		\vec{v}_i\neq\vec{v}_j\quad\text{if}\quad i\neq j.
		\] Then \(\set{\vec{v}_1,\dots,\vec{v}_n}\) is orthogonal basis of \(\R^n\).
		\item \(\lambda_1,\lambda_2,\dots,\lambda_k\) are distinct \(k\) eigenvalues with \(k<n\). Consider \begin{align*}
			C(\lambda_1)&:=\Span{\vec{v}_{1,1},\vec{v}_{1,2},\dots,\vec{v}_{1,n_1}},\\
			C(\lambda_2)&:=\Span{\vec{v}_{2,1},\vec{v}_{2,2},\dots,\vec{v}_{1,n_2}},\\
			&\vdots\\
			C(\lambda_k)&:=\Span{\vec{v}_{k,1},\vec{v}_{k,2},\dots,\vec{v}_{1,n_k}}.
		\end{align*} By Gram-Schmidt orthogonalization process, we have orthogonal basis of \(C(\lambda_i)\) as follows:
		\[
		\set{\textbf{w}_{1,1},\cdots,\vec{w}_{1,n},\cdots,\textbf{w}_{k,1},\cdots,\vec{w}_{k,n_k}}.
		\] Note that \[
		\sum_{i=1}^k\dim C(\lambda_i)=n_1+\cdots+n_k=n
		\] if \textbf{A} is Hermitian.
	\end{enumerate}
	\end{proof}
	\vspace{4pt}
	\newpage
	\begin{tcolorbox}[colframe=thmcolor,title={\color{white}\bf Spectral Decomposition}]
		\begin{theorem}
			Let \(\textbf{A}\) be a real symmetric. Then \[
			\textbf{A}=\textbf{P}\textbf{D}\textbf{P}^T,
			\] where $\textbf{D}=\begin{bmatrix}
				\lambda_1 &&0\\ &\ddots&\\0&&\lambda_n
			\end{bmatrix}$ is diagonal and \(\textbf{P}\) orthogonal matrix.
		\end{theorem}
	\end{tcolorbox}
	\begin{proof}
		Let \(\lambda_1,\dots,\lambda_n\) are solutions, counting multiplicity, of \(\det(\textbf{A}-\lambda\textbf{I}_n)=0\), and let \(\vec{v}_1,\cdots,\vec{v}_n\) are eigenvectors corresponding to \(\lambda_1,\dots,\lambda_n\), respectively. Since \(\set{\vec{v}_1,\dots,\vec{v}_n}\) is orthogonal basis of \(\R^n\), \[
		\textbf{P}:=\begin{bmatrix}
			\vec{v}_1&\cdots&\vec{v}_n
		\end{bmatrix}
		\] be a orthogonal matrix, and so \(\textbf{P}=\textbf{P}^T\). Then 
		\begin{align*}\textbf{A}\textbf{P}=\textbf{A}\begin{bmatrix}
				\vec{v}_1&\cdots&\vec{v}_n
			\end{bmatrix}&=\begin{bmatrix}
				\textbf{A}\vec{v}_1&\cdots&\textbf{A}\vec{v}_n
			\end{bmatrix}=\begin{bmatrix}
			\lambda_1\vec{v}_1&\cdots&\lambda_n\vec{v}_n
		\end{bmatrix}\\
		&=\begin{bmatrix}
			\vec{v}_1&\cdots&\vec{v}_n
		\end{bmatrix}\begin{bmatrix}
			\lambda_1 &&0\\ &\ddots&\\0&&\lambda_n
		\end{bmatrix}\\
		&=\textbf{P}\textbf{D}.
		\end{align*} Hence \[
		\textbf{AP}=\textbf{PD}\implies\textbf{A}=\textbf{PD}\textbf{P}^{-1}=\textbf{PD}\textbf{P}^T.
		\]
	\end{proof}
	\begin{remark}
		Let \(\textbf{A}\) be a real symmetric matrix. Then
		\begin{align*}
			\textbf{A}=\textbf{PD}\textbf{P}^T&=\begin{bmatrix}
				\vec{v}_1&\cdots&\vec{v}_n
			\end{bmatrix}\begin{bmatrix}
				\lambda_1 &&0\\ &\ddots&\\0&&\lambda_n
			\end{bmatrix}\begin{bmatrix}
			\vec{v}_1^T\\ \vdots\\\vec{v}_n^T
		\end{bmatrix}\\
		&=\begin{bmatrix}
			\vec{v}_1\lambda_1&\dots&\lambda_n\vec{v}_n
		\end{bmatrix}\begin{bmatrix}
			\vec{v}_1^T\\ \vdots\\\vec{v}_n^T
		\end{bmatrix}\\
		&=\sum_{i=1}^n\lambda_i\vec{v}_i\vec{v}_i^T.
		\end{align*}
		\begin{itemize}
			\item We call \(\lambda_i[\vec{v}_i\vec{v}_i^T]\) the principal component as an approximation of \(\textbf{A}\).
		\end{itemize}
	\end{remark}
	\newpage
	\chapter{Probability and Distributions}
	
	\begin{tcolorbox}[colframe=defcolor,title={\color{white}\bf }]
		\begin{definition}
			
		\end{definition}
	\end{tcolorbox}
	\begin{tcolorbox}[colframe=thmcolor,title={\color{white}\bf }]
		\begin{theorem}
			
		\end{theorem}
	\end{tcolorbox}
	
	\chapter{Continuous Optimization}
	
	\chapter{When Models Meet Data}
	
	\chapter{Linear Regression}
	
	\chapter{Dimensionality Reduction with PCA}
	% End document
\end{document}
