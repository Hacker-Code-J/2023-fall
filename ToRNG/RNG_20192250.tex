\documentclass[12pt,openany]{book}

\usepackage{amsmath,amsthm,amsfonts,amscd} % Packages for mathematics
\usepackage{commath} %absolute value
% Colors
\usepackage[dvipsnames]{xcolor}
\definecolor{titleblue}{RGB}{0,53,128}
\definecolor{chaptergray}{RGB}{140,140,140}
\definecolor{sectiongray}{RGB}{180,180,180}

\definecolor{thmcolor}{RGB}{231, 76, 60}
\definecolor{defcolor}{RGB}{52, 152, 219}
\definecolor{lemcolor}{RGB}{155, 89, 182}
\definecolor{corcolor}{RGB}{46, 204, 113}
\definecolor{procolor}{RGB}{241, 196, 15}

% Fonts
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{newpxtext,newpxmath}
\usepackage{sectsty}
\allsectionsfont{\sffamily\color{titleblue}\mdseries}

% Page layout
\usepackage{geometry}
\geometry{a4paper,left=1.1in,right=.7in,top=1in,bottom=1in,heightrounded}
\usepackage{fancyhdr}
\fancyhf{}
\fancyhead[LE,RO]{\thepage}
\fancyhead[LO]{\nouppercase{\rightmark}}
\fancyhead[RE]{\nouppercase{\leftmark}}
\renewcommand{\headrulewidth}{0.5pt}
\renewcommand{\footrulewidth}{0pt}

% Chapter formatting
\usepackage{titlesec}
\titleformat{\chapter}[display]
{\normalfont\sffamily\Huge\bfseries\color{titleblue}}{\chaptertitlename\ \thechapter}{20pt}{\Huge}
\titleformat{\section}
{\normalfont\sffamily\Large\bfseries\color{titleblue!100!gray}}{\thesection}{1em}{}
\titleformat{\subsection}
{\normalfont\sffamily\large\bfseries\color{titleblue!75!gray}}{\thesubsection}{1em}{}

% Table of contents formatting
\usepackage{tocloft}
\renewcommand{\cftchapfont}{\sffamily\color{titleblue}\bfseries}
\renewcommand{\cftsecfont}{\sffamily\color{chaptergray}}
\renewcommand{\cftsubsecfont}{\sffamily\color{sectiongray}}
\renewcommand{\cftchapleader}{\cftdotfill{\cftdotsep}}

\usepackage{cancel}
\newcommand\crossout[3][black]{\renewcommand\CancelColor{\color{#1}}\cancelto{#2}{#3}}
\newcommand\ncrossout[2][black]{\renewcommand\CancelColor{\color{#1}}\cancel{#2}}
% Hyperlinks
\usepackage{hyperref}
\hypersetup{
	colorlinks=true,
	linkcolor=titleblue,
	filecolor=black,      
	urlcolor=titleblue,
}

%Listing
\usepackage{listings} %Code
\renewcommand{\lstlistingname}{Code}%

\definecolor{sagegreen}{rgb}{0.0,0.6,0.4}
\definecolor{sagepurple}{rgb}{0.6,0.0,0.4}
\definecolor{sageblue}{rgb}{0.0,0.4,0.6}
\definecolor{sageorange}{rgb}{1.0,0.4,0.0}
\definecolor{sagegray}{rgb}{0.4,0.4,0.4}

\lstdefinestyle{sage}{
	language=Python,
	backgroundcolor=\color{white},
	basicstyle=\small\ttfamily\color{black}, 
	basicstyle=\footnotesize\ttfamily\color{black},
	keywordstyle=\color{blue!60!black},
	commentstyle=\color{green!60!black},
	stringstyle=\color{purple!60!black},
	showstringspaces=false,
	breaklines=true,
	tabsize=4,
	morekeywords={True, False, None},
	frame=leftline, % Remove the border
	framesep=3pt,
	frameround=tttt,
	framexleftmargin=3pt,
	numbers=left,
	numberstyle=\small\color{gray},
	xleftmargin=15pt, % Increase the left margin
	xrightmargin=5pt,
	captionpos=b,
	belowskip=0pt,
	aboveskip=4pt
}

%Ceiling and Floor Function
\usepackage{mathtools}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\DeclarePairedDelimiter{\floor}{\lfloor}{\rfloor}

%Algorithm
\usepackage[ruled,linesnumbered]{algorithm2e}
\usepackage{setspace}
\usepackage{algpseudocode}
\SetKwComment{Comment}{/* }{ */}
\SetKw{Break}{break}
\SetKw{Downto}{downto}
\SetKwProg{Fn}{Function}{:}{end}
\SetKwFunction{KeyGen}{KeyGen}


%---------------------------My Preamble
\usepackage{marvosym} %Lightning
\usepackage{booktabs}
\usepackage{multicol}
\usepackage{tabularx}
\setlength{\columnsep}{2cm}
\setlength{\columnseprule}{1.25pt}
\usepackage{enumerate}
\usepackage{soul}
\newcommand{\mathcolorbox}[2]{\colorbox{#1}{$\displaystyle #2$}}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{tikz-cd}
\usetikzlibrary{calc}
\usetikzlibrary{arrows, arrows.meta, positioning, shapes.multipart}

%Tcolorbox
\usepackage[most]{tcolorbox}
\tcbset{colback=white, arc=5pt}
%\tcbset{enhanced, colback=white,colframe=black,fonttitle=\bfseries,arc=4mm,boxrule=1pt,shadow={2mm}{-1mm}{0mm}{black!50}}
%White box with black text and shadow
%\begin{tcolorbox}[colback=white,colframe=black,fonttitle=\bfseries,title=Black Shadow Box,arc=4mm,boxrule=1pt,shadow={2mm}{-1mm}{0mm}{black!50}]
%	This is a white box with black text and a subtle shadow. The shadow adds some depth and dimension to the box without overpowering the design.
%\end{tcolorbox}

%Theorem
%\newtheorem{axiom}{Axiom}[chapter]
\newtheorem{theorem}{Theorem}[chapter]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}

\theoremstyle{definition}
\newtheorem{definition}{Definition}[chapter]
\newtheorem{remark}{Remark}[chapter]
\newtheorem{exercise}{Exercise}[chapter]
\newtheorem{example}{Example}[chapter]
\newtheorem*{note}{Note}
\newtheorem*{axiom}{Axiom}

%New Command
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\F}{\mathbb{F}}

\newcommand{\dispsty}{\displaystyle}
\newcommand{\sol}{\textcolor{magenta}{\bf Solution}}
\newcommand{\eg}{\textnormal{e.g.}}
\newcommand{\ie}{\textnormal{i.e.}}

\newcommand{\Var}{\text{Var}}
\newcommand{\sd}{\text{sd}}
\newcommand{\mean}[1]{\bar{#1}}
\newcommand{\Cov}{\textit{Cov}}
\newcommand{\Corr}{\textit{Corr}}
\newcommand{\SE}{\text{S.E.}}
\newcommand{\df}{\text{d.f.}}

\newcommand{\markov}[1]{\langle #1\rangle}
\newcommand{\tab}{\hspace{12pt}}

% Begin document
\begin{document}
	
	% Title page
	\begin{titlepage}
		\begin{center}
			{\Huge\textsf{\textbf{Theory of Random Number Generation}}\par}
			\vspace{0.5in}
			{\Large Ji Yong-Hyeon\par}
			\vspace{1in}
			%\includegraphics[width=2.5in]{rng.jpg}\par
			\vspace{1in}
			{\bf Department of Information Security, Cryptology, and Mathematics\par}
			{College of Science and Technology\par}
			{Kookmin University\par}
			%\includegraphics[width=1.5in]{school_logo.jpg}\par
			\vspace{.25in}
			{\large \today\par}
		\end{center}
	\end{titlepage}
	
	% Table of contents
	\tableofcontents
	
	% Chapters
	\mainmatter
	
	\chapter{Introduction}
	\begin{tcolorbox}[colback=white,colframe=defcolor,arc=5pt,title={\color{white}\bf Summary}]
		\begin{itemize}
			\item Required Properties for Random Bit Generator
			\begin{itemize}
				\item \textbf{Unpredictability}, \textbf{Unbiasedness}, \textbf{Independence}
			\end{itemize}
			\item Components of Cryptographically Secure Random Bit Generator
			\begin{itemize}
				\item TRNG (Entropy Source) + PRNG (Pseudorandom Number Generator)
			\end{itemize}
			\item Methods for Evaluating the Security of Random Bit Generator
			\begin{itemize}
				\item Estimation of entropy for the output sequence from TRNG
				\item Statistical randomness tests for the output sequence from RNG
			\end{itemize}
			\item Types of Random Bit Generators
			\begin{itemize}
				\item Hardware/Software-based Random Bit Generators
				\item Operating System-based Random Bit Generators
				\item Various Standard Pseudorandom Number Generators
			\end{itemize}
		\end{itemize}
	\end{tcolorbox}
	Functions of RBG (Random Bit Generator)
	
	Provides random numbers required for cryptographic systems
	An essential element (algorithm) for the operation of cryptographic systems and modules
	Required Properties: Unpredictability, Unbiasedness, Independence between bits
	
	Ideally, the output should be akin to the results of "coin tossing."
	Applications of Random Bit Generator
	
	Generation of Key and Initialization Vector (IV) used in symmetric-key cryptography (block/stream ciphers)
	Generation of various parameters in public-key cryptography: prime number generation, probabilistic public-key cryptography, etc.
	Generation of various parameters used in cryptographic protocols: nonce, salt, etc.
	
	\newpage
	\chapter{Probability Theory}
	
	\section{Introduction}
	
	\begin{tcolorbox}[colback=white,colframe=defcolor,arc=5pt,title={\color{white}\bf }]
		\begin{definition}
			\ \begin{itemize}
				\item An \textbf{experiment} is the process of observing a phenomenon that has variation in its outcomes.
				\item The \textbf{sample space} $S$ associated with an experiment is \underline{the collection} of all
				possible distinct outcomes of the experiment.
				\item An \textbf{event} $A, B$ is the set of elementary outcomes possessing a designated feature. ($A,B\subseteq S$)
			\end{itemize}
		\end{definition}
	\end{tcolorbox}
	\begin{remark}
		\ \begin{itemize}
			\item Union: $A\cup B$
			\item Complement: $A^C$
			\item Intersection: $A\cap B$ (simply, $AB$)
			\item $A,B$ are mutually disjoint $\iff A\cap B=\emptyset$
		\end{itemize}
	\end{remark}
	
	\newpage
	\section{Axioms of Probability}
	\subsection{Kolmogorov's Axiom}
	\begin{tcolorbox}[colback=white,colframe=magenta,arc=5pt,title={\color{white}\bf Kolmogorov's Axiom}]
		\begin{axiom}
			The probability is a function $\Pr:2^\Omega\to[0,1]\subseteq\R$ satisfies
			\begin{enumerate}[(\text{A}1)]
				\item $\forall$\text{event} $A$, $0\leq\Pr[A]\leq 1$.
				\item $\Pr[\Omega]=1$.
				\item (Countable Additivity) $
				P\del{\bigcup_{i=1}^\infty A_i}=\sum_{i=1}^\infty P[A_i],
				$ where $\set{A_1,A_2,\dots}$ is a countable set.
			\end{enumerate}
		\end{axiom}
	\end{tcolorbox}
	\begin{remark}
		A probability is a function $\Pr:2^{\Omega}\to[0,1]\subseteq\R$.
	\end{remark}
	\vspace{20pt}
	\begin{tcolorbox}[colback=white,colframe=procolor,arc=5pt,title={\color{white}\bf}]
		\begin{proposition}
			Let $A,B\subseteq\Omega$. \begin{enumerate}[(1)]
				\item $\Pr[A]=\Pr[AB^C]+\Pr[AB]$
				\item $\Pr[B]=\Pr[AB]+\Pr[A^CB]$
				\item $\Pr[A\cup B]=\Pr[A]+\Pr[B]-\Pr[AB]$
				\item $\Pr[A\cup B]=\Pr[AB^C]+\Pr[AB]+\Pr[A^CB]$
				\item $\Pr[A^C]=1-\Pr[A]$
				\item $A\subseteq B\implies\Pr[A]\leq\Pr[B]$
			\end{enumerate}
		\end{proposition}
	\end{tcolorbox}
	
	\newpage
	\subsection{Conditional Probability and Independent}
	
	\begin{tcolorbox}[colback=white,colframe=defcolor,arc=5pt,title={\color{white}\bf Conditional Probability}]
		\begin{definition}
			The \textbf{conditional probability} of $A$ given $B$ is denoted by $\Pr[A|B]$ and defined by the formula
			\[
			\Pr[A|B] = \frac{\Pr[AB]}{\Pr[B]}\quad\text{with}\quad\Pr[B]>0.
			\] Equivalently, this formula can be written as \textbf{multiplication law of probability}:\[
			\Pr[AB] = \Pr[A|B]\Pr[B].
			\]
		\end{definition}
	\end{tcolorbox}
	\begin{example}
	\ \begin{enumerate}[(1)]
		\item Start with a \textit{shuffled deck of cards} and distribute all 52 cards to 4 player, 13 cards to each. What is the probability that each player gets an Ace?
		\item Next, assume that you are a player and you get a single Ace. What is the probability now that each player gets an Ace?
	\end{enumerate}
	\begin{proof}[\sol]
		\ \begin{enumerate}[(1)]
			\item If any ordering of cards is equally likely, then any position of the four Aces in the deck is also equally likely. There are \[
			\binom{52}{4}=\frac{52!}{4!48!}
			\] possibilities for the positions (slots) for the 4 aces. Out of these, the number of positions that give each player an Ace $13^4$ pick the first slot among the cards that the first player gets, then the second slot among the second player's card, then the third and the fourth slot. Therefore, the answer is $
			\frac{13^4}{\binom{52}{4}}\approx0.1055.
			$
			\item After you see that you have a single Ace, the probability goes up the previous answer need to be divided by the probability that you get a single Ace, which is \[
			\frac{\dispsty13\cdot\binom{39}{3}}{\dispsty\binom{52}{4}}\approx0.4388.
			\] Note that \[
			P(A|B)=\frac{P(A\cap B)}{P(B)}=\frac{P(A)}{P(B)}.
			\] The answer then becomes $
			\frac{13^4}{13\cdot\binom{39}{3}}\approx0.2404.
			$
		\end{enumerate}
	\end{proof}
\end{example}
	\vspace{10pt}
	\begin{tcolorbox}[colback=white,colframe=defcolor,arc=5pt,title={\color{white}\bf Independence}]
		\begin{definition}
			Two events $A$ and $B$ are \textbf{independent} if \[
			\Pr[A|B] = \Pr[A]
			\] Equivalent conditions are \[
			\Pr[B|A] = \Pr[B]\quad\text{or}\quad \Pr[AB]=\Pr[A]\Pr[B]
			\]
		\end{definition}
	\end{tcolorbox}
	\begin{remark}
		$\displaystyle\Pr[A]=\Pr[A|B]=\frac{\Pr[AB]}{\Pr[B]}\implies \Pr[AB]=\Pr[A]\Pr[B]$.
	\end{remark}
	\vspace{10pt}
	\begin{example}
		Suppose we roll a dice once. Let the universal set is $U=\set{1,2,3,4,5,6}$.
		\begin{enumerate}[(1)]
			\item (Independent but Not Disjoint) Let \[
			A=\set{1,3,5}\quad\text{and}\quad B=\set{3,6}.
			\] Then $A\cap B=\set{3}\neq\emptyset$, that is, $A$ and $B$ are not disjoint. Note that \begin{align*}
				\Pr[A]=\frac{3}{6}=\frac{1}{2},\quad&\Pr[B]=\frac{2}{6}=\frac{1}{3},\\
				\Pr[A\mid B]=\frac{\Pr[AB]}{\Pr[B]}=\frac{1/6}{1/3}=\frac{1}{2},\quad&\Pr[B\mid A]=\frac{\Pr[BA]}{\Pr[A]}=\frac{1/6}{1/2}=\frac{1}{3}.
			\end{align*} Thus, $\Pr[A|B]=\Pr[A]$ and $\Pr[B|A]=\Pr[B]$. That is, $A$ and $B$ are mutually independent.
			\item (Not Independent but Disjoint) Let \[
			A=\set{1,3,5}\quad\text{and}\quad B=\set{2,4,6}.
			\] Then $A\cap B=\emptyset$, that is, $A$ and $B$ are disjoint. Note that \begin{align*}
				\Pr[A]=\frac{3}{6}=\frac{1}{2},\quad&\Pr[B]=\frac{3}{6}=\frac{1}{2},\\
				\Pr[A\mid B]=\frac{\Pr[AB]}{\Pr[B]}=\frac{0}{1/2}=0,\quad&\Pr[B\mid A]=\frac{\Pr[BA]}{\Pr[A]}=\frac{0}{1/2}=0.
			\end{align*} Thus, $\Pr[A|B]\neq\Pr[A]$ and $\Pr[B|A]\neq\Pr[B]$. That is, $A$ and $B$ are not independent.
		\end{enumerate}
	\end{example}
	\vspace{20pt}
	\begin{tcolorbox}[colback=white,colframe=procolor,arc=5pt,title={\color{white}\bf Rule of Total Probailtiy}]
		\begin{proposition}
			Let events $A_1,\dots,A_n$ are satisfies
			\begin{enumerate}[(1)]
				\item $\Pr[A_i]>0$ for $i=1,\dots,n$
				\item $A_i\cap A_j=\emptyset$ for $i\neq j$
				\item $\bigcup_{i=1}^nA_i=\Omega$
			\end{enumerate} Then \begin{align*}
				\Pr[B]&=\sum_{i=1}^n\Pr[B|A_i]\Pr[A_i]\\
				&=\Pr[B|A_1]\Pr[A_1]+\Pr[B|A_2]\Pr[A_2]+\cdots+\Pr[B|A_n]\Pr[A_n].
			\end{align*}
		\end{proposition}
	\end{tcolorbox}
	\begin{proof}
		$B=B\cap\Omega=B\cap\del{\bigcup_{i=1}^nA_i}=\bigcup_{i=1}^n(B\cap A_i)$.
	\end{proof}
	\vspace{20pt}
	\subsection{Bayes' Theorem}
	\begin{tcolorbox}[colback=white,colframe=thmcolor,arc=5pt,title={\color{white}\bf Bayes' Theorem}]
		\begin{theorem}
			\[
			P(B|A) = \frac{P(A|B)P(B)}{P(A|B)P(B)+P(A|\bar{B})P(\bar{B})}
			\] The posterior probability of $\bar{B}$ is then $P(\bar{B}|A)=1-P(B|A)$.
		\end{theorem}
	\end{tcolorbox}
	\begin{remark}
		\[
		\Pr[B\mid A]=\frac{\Pr[A|B]\cdot\Pr[B]}{\Pr[A]}\iff\text{Posterior}=\frac{\text{Likelihood}\cdot\text{Prior}}{\text{Evidence}}.
		\]
	\end{remark}
	
	\newpage
	\section{Random Variables}
	\begin{tcolorbox}[colframe=defcolor,title={\color{white}\bf Random Variable}]
		\begin{definition}
			A \textbf{random variable} $X$ is real-valued function on $\Omega$ the space of outcomes: \[
			X:\Omega\to\mathbb{R}.
			\] In other words, a random variable is a number whose value depends upon the outcome of a random experiment.
		\end{definition}
	\end{tcolorbox}
	\begin{remark}
		Sometimes, when convenient, we also allow $X$ to have the value $\infty$ or, more rarely, $-\infty$.
	\end{remark}

	\subsection{Discrete Random Variables}
	\begin{tcolorbox}[colback=white,colframe=defcolor,arc=5pt,title={\color{white}\bf Discrete Random Variable}]
	\begin{definition}
		A \textbf{discrete random variable} $X$ has finitely or countably many values $$
		x_i\quad\text{for}\quad i=1,2,\cdots
		$$ and $$p(x_i)=P(X=x_i)$$ with $i=1,2,\cdots,$ is called the \textbf{probability mass function of $X$}.
	\end{definition}
\end{tcolorbox}
	\begin{remark}
		A probability mass function $p$ has the following properties:\begin{enumerate}[(1)]
			\item $x=x_i, i\in I\implies p(x)=\Pr[X=x_i]$
			\item $0\leq p(x)\leq 1$, $\sum_{x\in X}p(x)=1$.
			\item $\Pr[a<X\leq b]=\sum_{a<x\leq b}p(x)$.
		\end{enumerate}
	\end{remark}
	\vspace{10pt}
	\begin{tcolorbox}[colback=white,colframe=defcolor,arc=5pt,title={\color{white}\bf Discrete Probability Distribution}]
		\begin{definition}
			The \textbf{probability distribution} of a discrete of a random variable $X$ is described as the function \[
			f(x_i) = P(X=x_i)
			\] which gives the probability for each value and satisfies: \begin{enumerate}
				\item $0\leq f(x_i)\leq 1$ for each value $x_i$ of $X$
				\item \(\dispsty\sum_{i=1}^kf(x_i)=1 \)
			\end{enumerate}
		\end{definition}
	\end{tcolorbox}

	\vspace{10pt}
	\begin{tcolorbox}[colback=white,colframe=defcolor,arc=5pt,title={\color{white}\bf Expectation(Mean) and Standard Deviation of a Probability Distribution}]
		\begin{definition}
			\ \begin{itemize}
				\item The \textbf{mean} of $X$ or \textbf{population mean} \begin{align*}
					E[X] &= \mu \\
					&= \sum(\text{Value}\ \times\ \text{Probability})=\sum x_if(x_i)
				\end{align*} Here the sum extends over all the distinct values $x_i$ of $X$.
				\item The \textbf{Variance and Standard Deviation of $\boldsymbol{X}$}
			 is given by \begin{align*}
			\sigma^2 &=\Var[X]=\sum(x_i-\mu)^2f(x_i) \\
			\sigma &=\sd[X]= +\sqrt{\Var[X]}
		\end{align*}
				\item \textbf{Alternative Formula for Hand calculation:} \[
				\sigma^2=\sum x_i^2f(x_i) - \mu^2
				\]
			\end{itemize}
		\end{definition}
	\end{tcolorbox}
	\vspace{10pt}
	\begin{example}[\bf Calculating a Population Variance and Standard Deviation]
		Calculate the variance and the standard deviation of the distribution of $X$ that appears in the left two columns of below table.
		\begin{center}\renewcommand*{\arraystretch}{1.4}\begin{tabularx}{\textwidth}{c|c||XXXX||c}
				\toprule[1.2pt]
				$x$ & $f(x)$ & $xf(x)$ & $(x-\mu)$ & $(x-\mu)^2$ & $(x-\mu)^2f(x)$ & $x^2f(x)$\\
				\hline
				0&.1& 0 & -2&4&.4&0\\
				1&.2& .2& -1&1&.2&0.2\\
				2&.4& .8& 0&0&.0&1.6\\
				3&.2& .6& 1&1&.2&1.8\\
				4&.1& .4& 2&4&.4&1.6\\
				\hline
				Total&1.0&2.0 = $\mu$&&&$1.2=\sigma^2$&$5.2=\sum x^2f(x)$\\
			\end{tabularx}
		\end{center}\begin{align*}
			\Var(X)&=\sigma^2=1.2\quad& \sigma^2=5.2-(2.0)^2=1.2 \\
			\sd(X)&=\sigma=\sqrt{1.2}=1.095\quad& \sigma=\sqrt{1.2}=1.095
		\end{align*}
	\end{example}
	
	\newpage
	\subsection{Bernoulli}
	\begin{note}
		\ \begin{itemize}
			\item The sample space $S = \set{\ \text{S},\ \text{F}\ }$.
			\item The probability of success $p=P(S)$, the probability of failure $q=P(F)$.
			\item $0\leq p\leq 1$, $q=1-p$.
		\end{itemize}
	\end{note}
	\vspace{20pt}
	\begin{tcolorbox}[colback=white,colframe=defcolor,arc=5pt,title={\color{white}\bf Binomial Distribution}]
		\begin{definition}
			The \textbf{binomial distribution} with $n$ trails and success probability $p$ is described by the function \[
			f(x) = P[X=x] = \binom{n}{x}p^x(1-p)^{n-x}
			\] for the possible values $x = 0, 1, \cdots, n$.
		\end{definition}
	\end{tcolorbox}
	\begin{example}[\bf An Example of the Binomial Distribution]
		The elementary outcomes of 4 samples, the associated probabilities, and the value of $X$ are listed as follows. \begin{center}
			\begin{tabular}{ccc ccc ccc ccc ccc}
				& FFFF &&& SFFF &&& SSFF &&& SSSF &&& SSSS & \\
				& 	   &&& FSFF &&& SFSF &&& SSFS &&& & \\
				& 	   &&& FFSF &&& SFFS &&& SFSS &&& & \\
				& 	   &&& FFFS &&& FSSF &&& FSSS &&& & \\
				& 	   &&&      &&& FSFS &&& &&& & \\
				& 	   &&&      &&& FFSS &&& &&& & \\
		\end{tabular}\end{center}
		\begin{center}\renewcommand*{\arraystretch}{1.4}\begin{tabularx}{\textwidth}{X||c|c|c|c|c}
				\toprule[1.2pt]
				Value of $X$ & 0 & 1 & 2 & 3 & 4 \\
				\hline\hline
				Probability of each outcome & $q^4$ & $pq^3$ & $p^2q^2$ & $p^3q$ & $p^4$ \\
				\hline
				Number of outcomes & $1=\binom{4}{0}$ & $4=\binom{4}{1}$ & $6=\binom{4}{2}$ & $4=\binom{4}{1}$ & $1=\binom{4}{4}$ \\
				\bottomrule[1.2pt] 
		\end{tabularx}\end{center}
		\begin{center}\renewcommand*{\arraystretch}{1.4}\begin{tabularx}{\textwidth}{X||c|c|c|c|c}
				\toprule[1.2pt]
				Value $x$ & 0 & 1 & 2 & 3 & 4 \\
				\hline\hline
				Probability $f(x)$ & $\binom{4}{0}p^0q^4$ & $\binom{4}{1}p^1q^3$ & $\binom{4}{2}p^2q^2$ & $\binom{4}{1}p^3q^1$ & $\binom{4}{4}p^4q^0$ \\
				\bottomrule[1.2pt]
			\end{tabularx}
		\end{center}
	\end{example}
	\vspace{10pt}
	\begin{tcolorbox}[colback=white,colframe=defcolor,arc=5pt,title={\color{white}\bf Mean and Standard Deviation of the Binomial Distribution}]
		\begin{definition}
			\[
			X=X_1+X_2+\cdots+X_n\sim B(n,p)
			\] \begin{itemize}
				\item \(E[X]=E[X_1] + \cdots + E[X_n] = np \)
				\item \(\Var[X]=\Var[X_1] + \cdots + \Var[X_n] = npq \)
			\end{itemize} The binomial distribution with $n$ trials and success probability $p$ has \begin{align*}
			\text{Mean} &= np \\
			\text{Variance} &= npq = np(1-p) \\
			\text{sd} &= \sqrt{npq} \\
		\end{align*}
		\end{definition}
	\end{tcolorbox}
	\vspace{10pt}
	\begin{tcolorbox}[colback=white,colframe=defcolor,arc=5pt,title={\color{white}\bf Covariance and Correlation Coefficient of Two Random Variables}]
		\begin{definition}
			Let $X, Y$ be a random variables. Then \begin{enumerate}
				\item The covariance of them: \[\Cov(X,Y)=E[(X-\mu_1)(Y-\mu_2)] \]
				\item The correlation coefficient of them: \[\dispsty\Corr(X,Y)=E\left[\left(\frac{X-\mu_1}{\sigma_1}\right)\left(\frac{Y-\mu_2}{\sigma_2}\right)\right]=\frac{\Cov(X,Y)}{\sd(X)\sd(Y)} \]
			\end{enumerate}
		\end{definition}
	\end{tcolorbox}
	\begin{remark}
		Note that $-1\leq\Corr(X,Y)\leq 1$ and \begin{align*}
			\Cov(X,Y) &= E[(X-\mu_1)(Y-\mu_2) ] \\
			&= E[XY-\mu_2X-\mu_1Y+\mu_1\mu_2] \\
			&= E[XY] - \mu_2E[X] -\mu_1E[Y] +\mu_1\mu_2 \\
			&= E[XY] - \mu_1\mu_2.
		\end{align*} That is, $\Cov(X,Y)=E[XY]-\mu_1\mu_2$.
	\end{remark}
	\vspace{10pt}
	\begin{tcolorbox}[colback=white,colframe=procolor,arc=5pt,title={\color{white}\bf }]
		\begin{proposition}
			 \ \begin{enumerate}[(1)]
				\item \(\Cov(aX+b,cY+d) = ac\cdot\Cov(X,Y) \)
				\item \(\Corr(aX+b,cY+d)=\begin{cases}
					\Corr(X,Y) &: ac>0 \\
					-\Corr(X,Y) &: ac<0 \\
				\end{cases} \)
			\end{enumerate}
		\end{proposition}
	\end{tcolorbox}
	\begin{proof}
		\begin{enumerate}[(1)]
			\item \begin{align*}
				\Cov(aX+b,cY+d) &= E[(aX+b)-(a\mu_x+b)\cdot(cY+d-(c\mu_y+d))] \\
				&= E[a(X-\mu_x)\cdot c(Y-\mu_y)]
				= acE[(X-\mu_x)(Y-\mu_y)] \\
				&= ac\cdot\Cov(X,Y).
			\end{align*}
			\item Note that $\sigma_{aX+b}=\sqrt{\Var(aX+b)}=\sqrt{a^2\Var(X)}=\abs{a}\sigma_X$. Similarly $\sigma_{cY+d}=\abs{c}\sigma_Y$.\begin{align*}
				\Corr(aX+b,cY+d) = \frac{\Cov(aX+b,cY+d)}{\sigma_{aX+b}\sigma_{cY+d}} 
				= \frac{ac\cdot\Cov(X,Y)}{\abs{a}\sigma_X\abs{c}\sigma_Y}
				= \frac{ac}{\abs{ac}}\Corr(X,Y).
			\end{align*} Hence, $\Corr(aX+b,cY+d)=\begin{cases}
				\Corr(X,Y) &\text{if}\ ac>0 \\
				-\Corr(X,Y) &\text{if}\ ac<0
			\end{cases}$
		\end{enumerate}
	\end{proof}
	\begin{tcolorbox}[colback=white,colframe=procolor,arc=5pt,title={\color{white}\bf Distribution of Sum of Two Probability Variables}]
		\begin{proposition}
			\ \begin{enumerate}[(1)]
				\item \(\Var(X+Y)=\Var(X)+\Var(Y)+2\Cov(X,Y) \)
				\item \(\Var(X-Y)=\Var(X)+\Var(Y)-2\Cov(X,Y) \)
			\end{enumerate}	
		\end{proposition}
	\end{tcolorbox}
	\vspace{10pt}
	\begin{tcolorbox}[colback=white,colframe=procolor,arc=5pt,title={\color{white}\bf Two Probability Variables are Independent}]
		\begin{proposition}
			\ \begin{enumerate}[(1)]
			\item \(E[XY]=E[X]\cdot E[Y] \)
			\item \(\Cov(X,Y)=0, \Corr(X,Y)=0 \)
			\item \(\Var(X\pm Y)=\Var(X)+\Var(Y) \)
		\end{enumerate}
		\end{proposition}
	\end{tcolorbox}
	\begin{proof}
		\begin{enumerate}[(1)]
			\item \begin{align*}
				E[XY] &= \sum_{i=1}^\infty\sum_{j=1}^\infty x_iy_jp(x_i,y_j) \\
				&= \sum_{i=1}^\infty\sum_{j=1}^\infty x_iy_jp_1(x_i)p_2(y_j) \\
				&= \sum_{i=1}^\infty x_ip_1(x_i)\sum_{j=1}^\infty y_jp_2(y_j)  \\
				&=E[X]\cdot E[Y].
			\end{align*} 
			\item $\Cov(X,Y) = E[XY]-E[X]\cdot E[Y]=0$.
		\end{enumerate}
	\end{proof}
	
	\newpage
	\subsection{Continuous Random Variables}
	\begin{tcolorbox}[colback=white,colframe=defcolor,arc=5pt,title={\color{white}\bf Probability Density Function}]
		\begin{definition}
			The \textbf{probability density function} $f(x)$ describes the distribution of probability for a continuous random variable. It has the properties: \begin{enumerate}[(1)]
				\item The total area under the probability density curve is $1$.
				\item $P[a\leq X\leq b]$ = area under the probability density curve between $a$ and $b$.
				\item $f(x)\geq 0$ for all $x$.
			\end{enumerate}
		\end{definition}
	\end{tcolorbox}
	\begin{remark}
		With a continuous random variable, the probability that $X=x$ is \textbf{always} 0. It is only meaningful to speak about the probability that $X$ lies in an interval.
	\end{remark}
	\begin{remark}
		$p(x)$ is called \textbf{probability density function} of continuous random variable $X$ if $p(x)$ satisfies: \begin{enumerate}[(i)]
			\item $p(x)\geq 0$, $\dispsty\int_{-\infty}^{\infty}p(x)\ dx=1$,
			\item $P(a\leq X\leq b)=\dispsty\int_{a}^bp(x)\ dx$.
		\end{enumerate}
		Note that
		\begin{itemize}
			\item For any constant $c$, $\dispsty\int_{c}^cp(x)\ dx=0$.
			\item $P(a\leq X\leq b)=P(a< X\leq b)=P(a\leq X< b)=P(a< X< b)$.
		\end{itemize}
	\end{remark}
	\vspace{10pt}
	\begin{tcolorbox}[colback=white,colframe=defcolor,arc=5pt,title={\color{white}\bf Expectation of a Continuous Random Variable}]
		\begin{definition}
			\ \begin{itemize}
				\item Expectation(or Mean) of a Random Variable $X$\begin{itemize}
					\item a Discrete Random variable: $E[X]=\sum_{i=1}^\infty x_ip(x_i)$
					\item a Continuous Random variable: $E[X]=\int_{-\infty}^\infty xp(x)\ dx$
				\end{itemize}
				\item Expectation and Median of a Continuous Random Variable $X$\begin{itemize}
					\item Expectation($\mu=E[X]$): the balance point of the probability mass.
					\item Median: the value of $X$ that divides the area under the curve into halves.
				\end{itemize}
			\end{itemize}
		\end{definition}
	\end{tcolorbox}
	
	\subsection{Normal Random Variable}
	\begin{tcolorbox}[colback=white,colframe=defcolor,arc=5pt,title={\color{white}\bf Normal Random Variable}]
		\begin{definition}
			A random variable is \textbf{Normal with parameter $\mu\in\mathbb{R}$ and $\sigma^2>0$} or, in short, \textbf{$X$ is $N(\mu,\sigma^2)$}, if its density is the function given below. \[
			\text{Density}:\ f(x)=f_X(x)=\frac{1}{\sigma\sqrt{2\pi}}\exp\left[-\frac{(x-\mu)^2}{2\sigma^2}\right],
			\] where $x\in(-\infty,\infty)$.
		\end{definition}
	\end{tcolorbox}
	\vspace{10pt}
	\begin{tcolorbox}[colback=white,colframe=procolor,arc=5pt,title={\color{white}\bf }]
		\begin{proposition}
			\ \begin{enumerate}[(1)]
				\item For$f(x)=\dispsty\frac{1}{\sigma\sqrt{2\pi}}\exp\left[-\frac{(x-\mu)^2}{2\sigma^2}\right]$,\quad $
				\dispsty\int_{-\infty}^{\infty} f(x)\ dx=1.
				$
				\item $EX=\mu$.
				\item $\Var(X)=\sigma^2$.
			\end{enumerate}
		\end{proposition}
	\end{tcolorbox}
	\subsection{The Normal Approximation to the Binomial}
	
	\begin{tcolorbox}[colback=white,colframe=thmcolor,arc=5pt,title={\color{white}\bf The Normal Approximation to the Binomial}]
		\begin{theorem}
			When $np$ and $np(1-p)$ are both large, say, greater than 15, the binomial distribution is well approximated by the normal distribution having mean = $np$ and sd = $\sqrt{np(1-p)}$. That is, \[
			Z=\frac{X-np}{\sqrt{np(1-p)}}\ \text{is approximately}\ N(0,1).
			\]
		\end{theorem}
	\end{tcolorbox}
	\vspace{5pt}
	\begin{tcolorbox}[colback=white,colframe=defcolor,arc=5pt,title={\color{white}\bf Mean and Standard Deviation of $\overline{X}$}]
		\begin{definition}
			The distribution of the sample mean, based on a random sample of size $n$, has \begin{align*}
				E[\overline{X}] &=\mu&(=\text{Population mean}) \\
				\Var[\overline{X}] &=\frac{\sigma^2}{n}&\left(=\frac{\text{Population variance}}{\text{Sample size}}\right) \\
				\sd[\overline{X}] &=\frac{\sigma}{\sqrt{n}}&\left(=\frac{\text{Population standard deviation}}{\sqrt{\text{Sample size}}}\right) \\
			\end{align*} 
		\end{definition}
	\end{tcolorbox}
	
	\newpage
	\section{Central Limit Theorem}
	\subsection{CLT}
	\begin{tcolorbox}[colback=white,colframe=thmcolor,arc=5pt,title={\color{white}\bf Central Limit Theorem}]
		\begin{theorem}
			Assume that $X,X_1,X_2,\dots$ are independent, identically distributed random variables, with finite $\mu=EX$ and $\sigma^2=\Var[X]$. Then, \[
			\lim\limits_{n\to\infty}\Pr\sbr{\frac{\sum_{i=1}^nX_i-\mu n}{\sigma\sqrt{n}}\leq x}=\Pr\sbr{Z\leq x},
			\] where $Z$ is standard Normal.
		\end{theorem}
	\end{tcolorbox}
	
	\subsection{Laws of Large Numbers}
	
	\begin{tcolorbox}[colback=white,colframe=thmcolor,arc=5pt,title={\color{white}\bf Weak Law of Large Numbers}]
		\begin{theorem}
			Let $X_1,X_2,\dots$ be a sequence of independent and identically distributed random variables, each having finite mean $E[X_i]=\mu$ and variance $\sigma^2$. Then, for any $\varepsilon>0$, \[
			\lim\limits_{n\to\infty}\Pr\sbr{\abs{\frac{\sum_{i=1}^n X_i}{n}-\mu}\geq\varepsilon}=0.
			\]
		\end{theorem}
	\end{tcolorbox}
	\vspace{20pt}
	\begin{tcolorbox}[colback=white,colframe=thmcolor,arc=5pt,title={\color{white}\bf Strong Law of Large Numbers}]
		\begin{theorem}
			Let $X_1,X_2,\dots$ be i.i.d. random variables with a finite first moment, $\mathbb{E}[X_i]=\mu$. Then \[
			\frac{1}{n}\sum_{i=1}^nX_i\to\mu\quad\text{almost surely as}\quad n\to\infty.
			\]
		\end{theorem}
	\end{tcolorbox}
	
	\newpage
	\section{Problem: RBG $\rightarrow$ RNG}
	\begin{exercise}[Uniform Distribution]
		Consider an algorithm 
		\begin{figure}[h!]
			\centering
			\begin{tabularx}{\textwidth}{cl}
				Step 1:& Drive the RBG independently $4$ times to generate a $4$-bit integer value $r$.\\
				Step 2:& \textbf{If} $r<10$ \textbf{then}\\
				&\tab\textbf{return} $r$\\
				&\textbf{else}\\
				&\tab \textbf{go to} Step 1
			\end{tabularx}
		\end{figure}\\
		Prove that \[
		\Pr[\mathsf{ouput}=n]=\frac{1}{10}
		\] for $n=0,1,2,\dots, 9$.
		\begin{proof}[\sol]
			Let \begin{itemize}
				\item $n\leq 2^k=m$ with $n=10, k=4$ and $m = 16$
				\item Output digit $=r\in[0,9]$.
			\end{itemize} Then \begin{table}[h!]\centering
				\begin{tabularx}{\textwidth}{c||c|c|c|c}
					\toprule[1.2pt]
					$\mathsf{output}$ & \textbf{1st iteration} & \textbf{2nd iteration} & $\cdots$ &\textbf{step iteration}\\
					\midrule
					$0$ & $\Pr[0]=\frac{1}{m}$ & $\Pr[0]=\frac{1}{m}\cdot\frac{m-n}{m}$ & $\cdots$ & $\Pr[0]=\frac{1}{m}\cdot\del{\frac{m-n}{m}}^{\textbf{step}-1}$\\
					$1$ & $\Pr[1]=\frac{1}{m}$ & $\Pr[1]=\frac{1}{m}\cdot\frac{m-n}{m}$ & $\cdots$ & $\Pr[1]=\frac{1}{m}\cdot\del{\frac{m-n}{m}}^{\textbf{step}-1}$\\
					$2$ & $\Pr[2]=\frac{1}{m}$ & $\Pr[2]=\frac{1}{m}\cdot\frac{m-n}{m}$ & $\cdots$ & $\Pr[2]=\frac{1}{m}\cdot\del{\frac{m-n}{m}}^{\textbf{step}-1}$\\
					$\vdots$&$\vdots$&$\vdots$&$\cdots$&$\vdots$\\
					$n-1$ & $\Pr[n-1]=\frac{1}{m}$ & $\Pr[n-1]=\frac{1}{m}\cdot\frac{m-n}{m}$ & $\cdots$ & $\Pr[n-1]=\frac{1}{m}\cdot\del{\frac{m-n}{m}}^{\textbf{step}-1}$\\
					\bottomrule[1.2pt]
				\end{tabularx}
			\end{table}\\ Thus, \begin{align*}
				\Pr[\mathsf{output}=r]&=\frac{1}{m}+\frac{1}{m}\cdot\frac{m-n}{m}+\cdots+\frac{1}{m}\cdot\del{\frac{m-n}{m}}^s+\cdots\\
				&=\frac{1}{m}\sum_{s=0}^\infty\del{\frac{m-n}{m}}^s=\frac{1}{m}\sum_{s=0}^\infty\del{1-\frac{n}{m}}^s\\
				&=\frac{1}{m}\cdot\frac{1}{1-\del{1-\frac{n}{m}}}=\frac{1}{m}\cdot\frac{m}{n}\\
				&=\frac{1}{n}.
			\end{align*}
		\end{proof}
	\end{exercise}

	\newpage
	\chapter{Markov Chains}
	
	\section{Introduction}
	\begin{tcolorbox}[colback=white,colframe=defcolor,arc=5pt,title={\color{white}\bf Markov Chain}]
		\begin{definition}
			Let $$
			\langle X_n\rangle_{n\geq 0}:=\set{X_n:n=0,1,2,\cdots}
			$$ be a stochastic process over a countable set $S$. Let $\Pr[X]$ is the probability of the random variable $X$. Then $\langle X_n\rangle_{n\geq 0}$ satisfies \textbf{Markov property} if \[
			\Pr[X_{n+1}=x_{n+1}\mid X_0=x_0,\dots,X_n=x_n]=\Pr[X_{n+1}=x_{n+1}\mid X_n=x_n]
			\] for all $n\geq 0$ and all $x_0,\dots, x_{n+1}\in S$. Then $\markov{X_n}_{n\geq 0}$ is a \textbf{Markov chain}.
		\end{definition}
	\end{tcolorbox}
	\begin{remark}
		\ \begin{enumerate}[(1)]
			\item The conditional probability of $X_{i+1}$ is dependent only upon $X_i$, and upon no earlier values of $\markov{X_n}$
			\item the state of $\markov{X_n}$ in the future is unaffected by its history.
			\item The set $S$ is called the \textbf{state space} of the Markov chain.
			\item The conditional probabilities $\Pr[X_{n+1}= y\mid X_n = x]$ are called the \textbf{transition probabilities}.
			\item Markov chain having \textbf{stationary transition probabilities}, \ie, $\Pr(X_{n+1}=y\mid X_n=x),$ is independent of $n$.
		\end{enumerate}
	\end{remark}
	\vspace{20pt}
	\newpage
	\begin{example}[\textit{The general two-state Markov chain}]
		There are two states $0$ and $1$ with transitions \begin{itemize}
			\item $0\to 1$ with probability $p$
			\item $0\to 0$ with probability $1-p$
			\item $1\to 0$ with probability $q$
			\item $1\to 1$ with probability $1-q$.
		\end{itemize} Thus we have \begin{align*}
		\Pr\sbr{X_{n+1}=1\mid X_n=0} &=p,\\
		\Pr\sbr{X_{n+1}=0\mid X_n=1} &=q,
	\end{align*} and $\Pr[X_0=0]=\pi_0(0)$. Since there are only two states, $0$ and $1$, it follows immediately that \begin{align*}
	\Pr\sbr{X_{n+1}=0\mid X_n=0}=1-p,\\
	\Pr\sbr{X_{n+1}=1\mid X_n=1}=1-q,
\end{align*} and $\pi_0(1)=\Pr[X_0=1]=1-\pi_0(0)$. The transition matrix has two parameters $p,q\in[0,1]$: \[
	\begin{bmatrix}
		T_{00} & T_{01}\\
		T_{10} & T_{11}\\
	\end{bmatrix}=\begin{bmatrix}
	1-p & p\\
	q & 1-q
\end{bmatrix}.
	\] Note that \begin{itemize}
		\item $\Pr[A]=\Pr[B\cap A]+\Pr[B^C\cap A]$
		\item $\Pr[A\cap B]=\Pr[A]\cdot\Pr[B\mid A]$
	\end{itemize} Then we observe that \begin{align*}
		\Pr\sbr{X_{n+1}=0}=&\Pr\sbr{X_n=0\land X_{n+1}=0}+\Pr\sbr{X_n=1\land X_{n+1}=0}\\
		=&\Pr[X_n=0]\Pr[X_{n+1}=0\mid X_n=0]\\
		&+\Pr[X_n=1]\Pr[X_{n+1}=0\mid X_n=1]\\
		=&\Pr[X_n=0]\cdot (1-p)+\Pr[X_n=1]\cdot q\\
		=&(1-p)\cdot \Pr[X_n=0]+q\cdot \del{1-\Pr[X_n=0]}\\
		=&(1-p-q)\cdot\Pr[X_n=0]+q.
	\end{align*} Now \begin{align*}
	\Pr[X_0=0]&=\pi_0(0),\\
	\Pr[X_1=0]&=(1-p-q)\pi_0(0)+q,\\
	\Pr[X_2=0]&=(1-p-q)\Pr[X_1=0]+q\\&=(1-p-q)^2\pi_0(0)+q\del{1+(1-p-q)},\\
	\Pr[X_3=0]&=(1-p-q)\Pr[X_2=0]+q\\&=(1-p-q)^3\pi_0(0)+q\del{1+(1-p-q)+(1-p-q)^2},\\
	&\vdots\\
	\Pr[X_n=0]&=(1-p-q)^n\pi_0(0) + q\sum_{j=0}^{n-1}(1-p-q)^j.
\end{align*} In the trivial case $p=q=0$, it is clear that for all $n$ \[
	\Pr[X_n=0]=\pi_0(0)\quad\text{and}\quad \Pr[X_n=1]=\pi_0(1).
\]	Suppose that $p+q>0$. By the formula $\displaystyle\sum_{j=0}^{n-1}r^j=\frac{1-r^n}{1-r}$ for the sum of a finite geometric progression, \[
	\sum_{j=0}^{n-1}(1-p-q)^j=\frac{1-(1-p-q)^n}{p+q}.
	\] Thus, \begin{align*}
		\Pr[X_n=0]&=\frac{q}{p+q}+(1-p-q)^n\cdot\del{\pi_0(0)-\frac{q}{p+q}},\\
		\Pr[X_n=1]&=\frac{p}{p+q}+(1-p-q)^n\cdot\del{\pi_0(1)-\frac{p}{p+q}}.
	\end{align*} Suppose that $p,q\notin\set{0,1}$. Then \[
	0<p+q<2\implies\abs{1-p-q}\leq 1.
	\] Then \[
	\lim\limits_{n\to\infty}\Pr[X_n=0]=\frac{q}{p+q}\quad\text{and}\quad\lim\limits_{n\to\infty}\Pr[X_n=1]=\frac{p}{p+q}.
	\] Suppose we want to choose $\pi_0(0)$ and $\pi_0(1)$ so that $\Pr[X_n=0]$ and $\Pr[X_n=1]$ are independent of $n$. To do this, we should choose $\pi_0(0)=q/(p+q)$ and $\pi_0(1)=p/(p+q)$.
	Thus if $\langle X_n\rangle_{n\geq 0}$ start with the initial distribution \[
	\pi_0=\Pr[X_n=0]=\frac{q}{p+q}\quad\text{and}\quad\pi_0(1)=\frac{p}{p+q},
	\] then for all $n$ \[
	\Pr[X_n=0]=\frac{q}{p+q}\quad\text{and}\quad\Pr[X_n=1]=\frac{p}{p+q}.
	\]
	\end{example}

	\newpage
	\begin{example}
		Let $n=2$ and $x_0,x_1,x_2\in\set{0,1}$. Then \begin{align*}
			&\Pr[X_0=x_0,X_1=x_1,X_2=x_2]\\=&
			\Pr[X_0=x_0,X_1=x_1]\cdot\Pr[X_2=x_2\mid X_0=x_0,X_1=x_1]\\=&
			\Pr[X_0=x_0]\Pr[X_1=x_1\mid X_0=x_0]\cdot\Pr[X_2=x_2\mid X_0=x_0,X_1=x_1].
		\end{align*} If the Markov property is satisfied, then \[
		\Pr[X_2=x_2\mid X_0=x_0,X_1=x_1]=\Pr[X_2=2\mid X_1=x_1],
	\] which is determined by $p$ and $q$. In this case \[
	\Pr[X_0=x_0,X_1=x_1,X_2=x_2]=\Pr[X_0=x_0]\Pr[X_1=x_1\mid X_0=x_0]\Pr[X_2=x_2\mid X_1=x_1].
	\] Recall that the transition matrix with $p,q\in[0,1]$: \[
	\begin{bmatrix}
		T_{00} & T_{01}\\
		T_{10} & T_{11}\\
	\end{bmatrix}=\begin{bmatrix}
		1-p & p\\
		q & 1-q
	\end{bmatrix}.
	\] Then \begin{figure}[h!]\centering\renewcommand*{\arraystretch}{1.4}
		\begin{tabularx}{\textwidth}{XXX|c}
			\toprule[1.2pt]
			$x_0$ &$x_1$& $x_2$ & $\Pr[X_0=x_0,X_1=x_1,X_2=x_2]$\\
			\midrule
			0& 0& 0     &$\pi_0(0)(1-p)^2$\\
			\hline
			0& 0& 1     &$\pi_0(0)(1-p)p$\\
			\hline
			0& 1& 0     &$\pi_0(0)pq$\\
			\hline
			0& 1& 1     &$\pi_0(0)p(1-q)$\\
			\hline
			\hline
			1& 0& 0     &$(1-\pi_0(0))q(1-p)$\\
			\hline
			1& 0& 1     &$(1-\pi_0(0))qp$\\
			\hline
			1& 1& 0     &$(1-\pi_0(0))(1-q)q$\\
			\hline
			1& 1& 1     &$(1-\pi_0(0))(1-q)^2$\\
			\bottomrule[1.2pt]
		\end{tabularx}
	\end{figure}
	\end{example}
	
	% End document
\end{document}
